{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<span style=\"color:red; font-size:2em; font-weight: bold\">Social Media Network Analytics - Assignment 1</span>\n",
    "\n",
    "## __Name: Harry Marshall__\n",
    "## __Student ID: s4015845__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Collection__\n",
    "### Connect to Reddit Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reddit client that will gather all the data.\n",
    "from redditClient import redditClient\n",
    "\n",
    "# name of subreddit the data is collected from.\n",
    "subreddit_name= 'stocks'\n",
    "# retrieved_result_limit = 5000\n",
    "\n",
    "# # construct reddit client.\n",
    "# client = redditClient()\n",
    "\n",
    "# # access the subreddit.\n",
    "# subreddit = client.subreddit(subreddit_name)\n",
    "# # print subreddit title.\n",
    "# print(subreddit.title)\n",
    "# # print subreddit descp.\n",
    "# print(subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of Subreddit Submissions in a Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand the amount of reddit posts made in a month.\n",
    "# import praw\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # calculate date range.\n",
    "# end_date = datetime.utcnow()\n",
    "# start_date = end_date - timedelta(days=30)\n",
    "# start_timestamp = int(start_date.timestamp())\n",
    "\n",
    "# print(f\"Counting posts in r/{subreddit_name} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# # init post count.\n",
    "# post_count = 0\n",
    "\n",
    "# # use 'new' sorting to get posts.\n",
    "# for submission in subreddit.new(limit=None):\n",
    "#     # check if we're within our time range.\n",
    "#     if submission.created_utc < start_timestamp:\n",
    "#         # stop searching after a months worth of posts.\n",
    "#         break\n",
    "        \n",
    "#     post_count += 1\n",
    "    \n",
    "#     # print progress every 100 posts.\n",
    "#     if post_count % 100 == 0:\n",
    "#         print(f\"Counted {post_count} posts so far...\")\n",
    "\n",
    "# # print result.\n",
    "# print(f\"\\nRESULTS:\")\n",
    "# print(f\"Total posts in r/{subreddit_name} in the past 30 days: {post_count}\")\n",
    "# print(f\"Average posts per day: {post_count / 30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Structure Subreddit Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain months worth of reddit posts.\n",
    "# import json\n",
    "# from praw.models import MoreComments\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "\n",
    "# # create a json file name.\n",
    "# output_filename = f'../json/{subreddit_name}-submissions-raw.json'\n",
    "# submissions_json = []\n",
    "\n",
    "# # 750 posts have been made in a month.\n",
    "# for index, submission in enumerate(subreddit.new(limit=retrieved_result_limit)):\n",
    "#     if index % 100 == 0:\n",
    "#         print(f\"Collected {index + 1} submissions...\")\n",
    "\n",
    "#     submission_data = {\n",
    "#         \"submission_id\": submission.id,\n",
    "#         \"submission_date\": submission.created_utc,\n",
    "#         \"submission_title\": submission.title,\n",
    "#         \"submission_author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "#         \"submission_score\": submission.score,\n",
    "#         \"submission_upvote_ratio\": submission.upvote_ratio,\n",
    "#         \"submission_num_comments\": submission.num_comments,\n",
    "#         \"submission_text\": submission.selftext if submission.is_self else \"\",\n",
    "#         # Empty list to hold comments later if needed\n",
    "#         \"submission_comments\": [] \n",
    "#     }\n",
    "\n",
    "#     if submission.num_comments > 0:\n",
    "#         comments_list = [comment for comment in submission.comments.list() if not isinstance(comment, MoreComments)]\n",
    "\n",
    "#         for i, comment in enumerate(comments_list):\n",
    "\n",
    "#             if i >= 35:\n",
    "#                 break \n",
    "\n",
    "#             comment_data = {\n",
    "#                 \"comment_id\": comment.id,\n",
    "#                 \"comment_author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "#                 \"comment_score\": comment.score,\n",
    "#                 \"comment_text\": comment.body,\n",
    "#                 \"comment_num_replies\": len(comment.replies) if hasattr(comment, \"replies\") else 0\n",
    "#             }\n",
    "\n",
    "#             submission_data['submission_comments'].append(comment_data)\n",
    "\n",
    "#     submissions_json.append(submission_data)\n",
    "#     time.sleep(0.5)\n",
    "\n",
    "# # final header struct of json file.\n",
    "# final_data = {\n",
    "#     \"subreddit\": subreddit_name,\n",
    "#     \"collection_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#     \"total_posts\": len(submissions_json),\n",
    "#     \"posts\": submissions_json\n",
    "# }\n",
    "\n",
    "# # save to JSON file.\n",
    "# with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
    "#     print('json data saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Exploratory Data Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Unique and most Active Users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from praw.models import MoreComments\n",
    "# from collections import Counter\n",
    "\n",
    "# # specify which subreddit we are interested in.\n",
    "# subreddit = client.subreddit(subreddit_name)\n",
    "\n",
    "# l_post_author = []\n",
    "# l_comment_author = []\n",
    "\n",
    "# # obtain the hot submissions for this subreddit.\n",
    "# for submission in subreddit.hot(limit=750):\n",
    "#     # print title, but we can print other information of each submission as well.\n",
    "#     if submission.author is not None:\n",
    "#         l_post_author.append(submission.author.name)\n",
    "\n",
    "#     # check submission has comments.\n",
    "#     if submission.num_comments > 0:\n",
    "#         # remove all MoreComments class from list.\n",
    "#         comments_list = [comment for comment in submission.comments.list() if not isinstance(comment, MoreComments)]\n",
    "#         # stop after 35 comments.\n",
    "#         for i, comment in enumerate(comments_list):\n",
    "#             if i >= 35:\n",
    "#                 break\n",
    "\n",
    "#             if comment.author is not None:\n",
    "#                 l_comment_author.append(comment.author.name)\n",
    "\n",
    "# post_names_counter = Counter(l_post_author)\n",
    "# comment_names_counter = Counter(l_comment_author)\n",
    "\n",
    "# # print the results of post authors.\n",
    "# print(f\"Post submission authors: {post_names_counter}\")\n",
    "# print(f\"Number of Users: {len(post_names_counter)}\")\n",
    "\n",
    "# # print the results of comment authors.\n",
    "# print(f\"Post submission authors: {comment_names_counter}\")\n",
    "# print(f\"Number of Users: {len(comment_names_counter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Preprocessing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenisation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def process_text(text):\n",
    "\n",
    "    # Tweet tokeniser.\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    l_punct = list(string.punctuation)\n",
    "    # Add custom stopwords to the standard list\n",
    "    additional_stopwords = [\n",
    "        'would', 'could', 'also', 'like', 'one', 'go', 'see', 'think', 'make',\n",
    "        'get', 'take', 'look', 'know', 'want', 'say', 'said', 'even', 'still',\n",
    "        'much', 'many', 'back', 'good', 'new', 'this', 'that', 'these', 'those',\n",
    "        'has', 'had', 'was', 'were', 'been', 'being', 'have', 'any', 'some'\n",
    "    ]\n",
    "    stop_words = nltk.corpus.stopwords.words('english') + l_punct + ['via'] + additional_stopwords\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "    # Define stock ticker pattern.\n",
    "    ticker_pattern = r'\\$?[A-Z]{2,4}'\n",
    "    ticker_pattern_2 = r'[A-Z]{1,2}\\&[A-Z]{1,2}'\n",
    "\n",
    "    # Extract potential stock tickers before any processing.\n",
    "    potential_tickers = set(re.findall(ticker_pattern, text))\n",
    "    potential_tickers_2 = set(re.findall(ticker_pattern_2, text))\n",
    "    potential_tickers.update(potential_tickers_2)\n",
    "\n",
    "    # print known abbreviations. \n",
    "    print(potential_tickers)\n",
    "    \n",
    "    # convert to lower case, but after extracting tickers.\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # get rid of all urls.\n",
    "    text_lower = re.sub(r'https?://\\S+|www\\.\\S+', '', text_lower)\n",
    "    \n",
    "    # filter out emoticons and other noisy characters.\n",
    "    text_lower = ''.join(char for char in text_lower if char.isalnum() or char.isspace() or char in string.punctuation)\n",
    "    \n",
    "    # tokenise text.\n",
    "    l_tokens = tokenizer.tokenize(text_lower)\n",
    "    # store original length of text.\n",
    "    original_word_count = len(l_tokens)\n",
    "    \n",
    "    # strip whitespaces.\n",
    "    l_tokens = [token.strip() for token in l_tokens]\n",
    "    \n",
    "    # remove very short tokens and stopwords early, but keep potential stock tickers\n",
    "    l_tokens = [token for token in l_tokens if \n",
    "                (token.upper() in potential_tickers) or\n",
    "                (len(token) > 2 and  \n",
    "                token not in stop_words and \n",
    "                not token.isdigit() and \n",
    "                token != \"'\" and token != \"''\" and\n",
    "                token != \"...\" and token != \"â€¦\")]\n",
    "    \n",
    "    # now stem only the normal words, but leave stock tickers untouched.\n",
    "    l_stemmed_tokens = set()\n",
    "    for tok in l_tokens:\n",
    "        if tok.upper() in potential_tickers:\n",
    "            # keep tickers in uppercase to highlight them.\n",
    "            l_stemmed_tokens.add(tok.upper())\n",
    "        else:\n",
    "            # stem regular words.\n",
    "            l_stemmed_tokens.add(stemmer.stem(tok))\n",
    "    \n",
    "    # calc the word count after tokenisation.\n",
    "    final_word_count = len(l_stemmed_tokens)\n",
    "    \n",
    "    # calculate dropped words.\n",
    "    words_dropped = original_word_count - final_word_count\n",
    "    return list(l_stemmed_tokens), words_dropped, original_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new JSON file with Preprocessed Text\n",
    "##### Using process_text() function on natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# collected submission json file.\n",
    "input_filename = f'../datasets/{subreddit_name}-submissions-raw.json'\n",
    "output_filename = f'../datasets/{subreddit_name}-submissions-token.json'\n",
    "\n",
    "# number of most frequent terms to display.\n",
    "freq_num = 85\n",
    "\n",
    "# term freq counter.\n",
    "term_freq_counter = Counter()\n",
    "\n",
    "# attributes in the json struct that need to be tokenised.\n",
    "tokenised_submission = [\n",
    "    'submission_title',\n",
    "    'submission_text',\n",
    "]\n",
    "\n",
    "total_words_dropped = 0\n",
    "total_words_dataset = 0\n",
    "\n",
    "# open JSON file and process submissions.\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    d_submissions = json.load(f)\n",
    "\n",
    "    for submission in d_submissions['posts']:\n",
    "\n",
    "        for var in tokenised_submission:\n",
    "            submission_text = submission.get(var, '')\n",
    "            l_tokens, submission_words_dropped, submission_words = process_text(submission_text)\n",
    "            # replace original text with tokenized version.\n",
    "            submission[var] = ' '.join(l_tokens)\n",
    "            term_freq_counter.update(l_tokens)\n",
    "\n",
    "            total_words_dropped += submission_words_dropped\n",
    "            total_words_dataset += submission_words\n",
    "\n",
    "        for comment in submission['submission_comments']:\n",
    "            comment_text = comment.get('comment_text', '')\n",
    "            l_tokens, comment_words_dropped, comment_words = process_text(comment_text)\n",
    "            # replace original text with tokenized version.\n",
    "            comment['comment_text'] = ' '.join(l_tokens)\n",
    "\n",
    "            term_freq_counter.update(l_tokens)\n",
    "\n",
    "            total_words_dropped += comment_words_dropped\n",
    "            total_words_dataset += comment_words\n",
    "\n",
    "# save the tokenised data to a new JSON file.\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(d_submissions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Amount of words in text fields: {total_words_dataset}\")\n",
    "print(f\"Amount of words dropped: {total_words_dropped}\")\n",
    "# print out most common terms.\n",
    "ranking = 1\n",
    "for term, count in term_freq_counter.most_common(freq_num):\n",
    "    print(str(ranking) + ': ' + term + ': ' + str(count))\n",
    "    ranking += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the most common terms,\n",
    "most_common = term_freq_counter.most_common(freq_num)\n",
    "\n",
    "# create histogram of term frequencies.\n",
    "terms, counts = zip(*most_common)\n",
    "\n",
    "# create a bar chart.\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.bar(range(len(terms)), counts, align='center')\n",
    "plt.xticks(range(len(terms)), terms, rotation=90)\n",
    "plt.xlabel('Terms')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Term Frequency Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# create a histogram of frequency values to show the distribution pattern.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=20)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of Terms')\n",
    "plt.title('Distribution of Term Frequencies')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  __Method and Modelling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentiment Analysis and Slang Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "custom_lexicon = {\n",
    "    # postive terms.\n",
    "    'moon': 3.0,\n",
    "    'mooning': 3.0,\n",
    "    'bull': 2.0,\n",
    "    'bullish': 2.5,\n",
    "    'long': 1.0,\n",
    "    'calls': 1.5,\n",
    "    'hodl': 1.0,\n",
    "    'tendies': 2.0,\n",
    "    'rocket': 2.5,\n",
    "    'rocketship': 2.5,\n",
    "    'yolo': 1.5,\n",
    "    'buy the dip': 1.5,\n",
    "    'btd': 1.5,\n",
    "    'ath': 2.0,\n",
    "    'breakout': 1.5,\n",
    "    'outperform': 1.5,\n",
    "    'upgrade': 1.0,\n",
    "    'beat': 1.0,\n",
    "    \n",
    "    # negative terms.\n",
    "    'bear': -2.0,\n",
    "    'bearish': -2.5,\n",
    "    'short': -1.0,\n",
    "    'puts': -1.5, \n",
    "    'bagholder': -2.0,\n",
    "    'sell off': -1.5,\n",
    "    'dump': -2.0,\n",
    "    'crash': -3.0,\n",
    "    'recession': -2.5,\n",
    "    'drilling': -2.0,\n",
    "    'tanking': -2.5,\n",
    "    'guh': -2.0,\n",
    "    'rugpull': -3.0,\n",
    "    'fud': -1.5,\n",
    "    'underperform': -1.5,\n",
    "    'downgrade': -1.0,\n",
    "    'miss': -1.0,\n",
    "}\n",
    "    \n",
    "\n",
    "stemmed_lexicon = {}\n",
    "for word, score in custom_lexicon.items():\n",
    "    stemmed_tokens, _, _ = process_text(word)\n",
    "    \n",
    "    # join into a string if there are tokens\n",
    "    if stemmed_tokens:\n",
    "        stemmed_word = ' '.join(stemmed_tokens)\n",
    "        stemmed_lexicon[stemmed_word] = score\n",
    "\n",
    "# start VADER sentiment analyser.\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# update VADER's lexicon with our stemmed custom lexicon.\n",
    "sia.lexicon.update(stemmed_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Sentiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment(l_tokens):\n",
    "    \n",
    "    return sia.polarity_scores(l_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sentiment Analysis of Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "# import tokenised submission JSON file.\n",
    "input_filename = f'../datasets/{subreddit_name}-submissions-token.json'\n",
    "output_json = f'../datasets/{subreddit_name}-submissions-tf-idf.json'\n",
    "output_csv = f'../datasets/{subreddit_name}-submissions-tf-idf.csv'\n",
    "\n",
    "# variables for text fields.\n",
    "tokenised_submission = [\n",
    "    'submission_title',\n",
    "    'submission_text',\n",
    "]\n",
    "\n",
    "# dictionary to store submission data.\n",
    "submission_texts = {}\n",
    "submission_metadata = {}\n",
    "submission_topics = {}\n",
    "all_topics = set()\n",
    "\n",
    "# open JSON file and process submissions.\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    d_submissions = json.load(f)\n",
    "    \n",
    "    # extract text from each submission and its comments.\n",
    "    for submission in d_submissions['posts']:\n",
    "        submission_id = submission.get('submission_id', '')\n",
    "        combined_text = \"\"\n",
    "        sentiment_total = 0\n",
    "        num_sentiment_items = 0\n",
    "        \n",
    "        # get timestamp information.\n",
    "        created_utc = submission.get('submission_date', '')\n",
    "        if created_utc:\n",
    "            try:\n",
    "                date_time = datetime.datetime.fromtimestamp(float(created_utc))\n",
    "            except:\n",
    "                date_time = None\n",
    "        else:\n",
    "            date_time = None\n",
    "        \n",
    "        # get submission text fields.\n",
    "        for var in tokenised_submission:\n",
    "            submission_text = submission.get(var, '')\n",
    "            if submission_text:\n",
    "                sentiment_score = compute_sentiment(submission_text)\n",
    "                print(sentiment_score['compound'])\n",
    "                sentiment_total += sentiment_score['compound']\n",
    "                num_sentiment_items += 1\n",
    "                combined_text += \" \" + submission_text\n",
    "        \n",
    "        # get comment text fields.\n",
    "        for comment in submission.get('submission_comments', []):\n",
    "            comment_text = comment.get('comment_text', '')\n",
    "            if comment_text:\n",
    "                sentiment_score = compute_sentiment(comment_text)  # Changed from combined_sentiment to compute_sentiment\n",
    "                sentiment_total += sentiment_score['compound']\n",
    "                num_sentiment_items += 1\n",
    "                combined_text += \" \" + comment_text\n",
    "        \n",
    "        # calculate average sentiment.\n",
    "        avg_sentiment = sentiment_total / num_sentiment_items if num_sentiment_items > 0 else 0\n",
    "        \n",
    "        # store the combined text and metadata for this submission.\n",
    "        submission_texts[submission_id] = combined_text.strip()\n",
    "        submission_metadata[submission_id] = {\n",
    "            'date': date_time.strftime('%Y-%m-%d') if date_time else '',\n",
    "            'time': date_time.strftime('%H:%M:%S') if date_time else '',\n",
    "            'timestamp': date_time.timestamp() if date_time else None,\n",
    "            'sentiment': avg_sentiment,\n",
    "            'num_comments': len(submission.get('submission_comments', [])),\n",
    "            'topics': []  # Will be filled after TOPIC analysis.\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency) Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create corpus from all submission texts.\n",
    "corpus = list(submission_texts.values())\n",
    "submission_ids = list(submission_texts.keys())\n",
    "\n",
    "# create TF-IDF. \n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    # >80% of documents.\n",
    "    max_df=0.8,  \n",
    "    # min doc word count. \n",
    "    min_df=3,\n",
    "    # top 500 features.\n",
    "    max_features=500,\n",
    "    # ignore single words or phrases.\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# fit the vectorizer to the corpus.\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# get feature names.\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "# get top terms for each submission based on TF-IDF scores.\n",
    "for i, submission_id in enumerate(submission_ids):\n",
    "    # get the document vector.\n",
    "    doc_vector = tfidf_matrix[i].toarray()[0]\n",
    "    \n",
    "    # get indices of top 10 TF-IDF scores.\n",
    "    top_indices = doc_vector.argsort()[-10:][::-1]\n",
    "    \n",
    "    # get corresponding terms and their scores.\n",
    "    top_terms_with_scores = [(feature_names[idx], doc_vector[idx]) for idx in top_indices if doc_vector[idx] > 0]\n",
    "    top_terms = [term for term, score in top_terms_with_scores]\n",
    "\n",
    "    # store as the topic for this submission\n",
    "    submission_topics[submission_id] = top_terms\n",
    "    # update metadata\n",
    "    submission_metadata[submission_id]['topics'] = top_terms\n",
    "    \n",
    "    # add to overall set of topics\n",
    "    all_topics.update(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniGrams and BiGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create corpus from all submission texts\n",
    "corpus = list(submission_texts.values())\n",
    "submission_ids = list(submission_texts.keys())\n",
    "\n",
    "# Create a document-term matrix using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=1,\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2) # include unigrams and bigrams\n",
    ")\n",
    "# Fit the vectorizer to the corpus\n",
    "doc_term_matrix = count_vectorizer.fit_transform(corpus)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract unigrams and bigrams\n",
    "unigram_indices = [i for i, feature in enumerate(feature_names) if ' ' not in feature]\n",
    "bigram_indices = [i for i, feature in enumerate(feature_names) if ' ' in feature]\n",
    "\n",
    "# Get unigram frequencies\n",
    "unigram_names = [feature_names[i] for i in unigram_indices]\n",
    "unigram_counts = doc_term_matrix[:, unigram_indices].sum(axis=0).A1\n",
    "unigram_freq = dict(zip(unigram_names, unigram_counts))\n",
    "sorted_unigrams = sorted(unigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "# Get bigram frequencies\n",
    "bigram_names = [feature_names[i] for i in bigram_indices]\n",
    "bigram_counts = doc_term_matrix[:, bigram_indices].sum(axis=0).A1\n",
    "bigram_freq = dict(zip(bigram_names, bigram_counts))\n",
    "sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create a side-by-side visualization of unigrams and bigrams\n",
    "def plot_unigrams_and_bigrams(sorted_unigrams, sorted_bigrams, n=15):\n",
    "    \"\"\"Create side-by-side plots of top unigrams and bigrams.\"\"\"\n",
    "    # Get actual number to plot\n",
    "    n_unigrams = min(n, len(sorted_unigrams))\n",
    "    n_bigrams = min(n, len(sorted_bigrams))\n",
    "    \n",
    "    if n_bigrams == 0:\n",
    "        print(\"No bigrams to visualize!\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Get colorful palettes for bars\n",
    "    colors1 = sns.color_palette(\"husl\", n_unigrams)\n",
    "    colors2 = sns.color_palette(\"husl\", n_bigrams)\n",
    "    \n",
    "    # Plot unigrams\n",
    "    u_data = sorted_unigrams[:n_unigrams]\n",
    "    unigrams, u_counts = zip(*u_data)\n",
    "    bars1 = ax1.bar(range(n_unigrams), u_counts, width=0.7, color=colors1, edgecolor='none')\n",
    "    ax1.set_title('Top Unigrams (Single Words)', fontsize=14)\n",
    "    ax1.set_xticks(range(n_unigrams))\n",
    "    ax1.set_xticklabels(unigrams, rotation=45, ha='right', fontsize=10)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.grid(axis='y', linestyle='-', alpha=0.2)\n",
    "    \n",
    "    # Plot bigrams\n",
    "    b_data = sorted_bigrams[:n_bigrams]\n",
    "    bigrams, b_counts = zip(*b_data)\n",
    "    bars2 = ax2.bar(range(n_bigrams), b_counts, width=0.7, color=colors2, edgecolor='none')\n",
    "    ax2.set_title(f'Top Bigrams (Word Pairs)', fontsize=14)\n",
    "    ax2.set_xticks(range(n_bigrams))\n",
    "    ax2.set_xticklabels(bigrams, rotation=45, ha='right', fontsize=10)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.grid(axis='y', linestyle='-', alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot unigrams and bigrams side by side\n",
    "plot_unigrams_and_bigrams(sorted_unigrams, sorted_bigrams, n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of topics for LDA\n",
    "num_topics = 15\n",
    "# Create and fit the LDA model\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=num_topics,\n",
    "    max_iter=10,\n",
    "    learning_method='online',\n",
    "    random_state=42,\n",
    "    batch_size=128,\n",
    "    learning_decay=0.7\n",
    ")\n",
    "# Fit the model to the document-term matrix\n",
    "lda_model.fit(doc_term_matrix)\n",
    "# Function to print top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_terms = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(topic_terms)\n",
    "        print(f\"Topic #{topic_idx+1}: {', '.join(topic_terms)}\")\n",
    "    return topics\n",
    "# Print top words for each topic\n",
    "print(\"\\nTopics found by LDA:\")\n",
    "all_topics_list = print_top_words(lda_model, feature_names)\n",
    "# Get topic distribution for each document\n",
    "topic_distributions = lda_model.transform(doc_term_matrix)\n",
    "# Assign the most probable topic to each submission\n",
    "submission_topics = {}\n",
    "all_topics = set()\n",
    "for i, submission_id in enumerate(submission_ids):\n",
    "    # Get the most probable topic for this document\n",
    "    dominant_topic_idx = np.argmax(topic_distributions[i])\n",
    "    # Get the top terms for this topic\n",
    "    top_terms = all_topics_list[dominant_topic_idx]\n",
    "    # Store as the topic for this submission\n",
    "    submission_topics[submission_id] = top_terms\n",
    "    # Update metadata (if submission_metadata exists)\n",
    "    if 'submission_metadata' in globals():\n",
    "        submission_metadata[submission_id]['topics'] = top_terms\n",
    "    # Add top terms to overall set of topics\n",
    "    all_topics.update(top_terms)\n",
    "# Count the frequency of topics across all submissions\n",
    "topic_counts = {}\n",
    "for topics in submission_topics.values():\n",
    "    for topic in topics:\n",
    "        if topic in topic_counts:\n",
    "            topic_counts[topic] += 1\n",
    "        else:\n",
    "            topic_counts[topic] = 1\n",
    "# Print the most common topics\n",
    "print(\"\\nMost Common Topics:\")\n",
    "for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{topic}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Focused Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a topic-oriented view.\n",
    "topic_analysis = {}\n",
    "for topic in all_topics:\n",
    "    # find all submissions containing this topic.\n",
    "    relevant_submissions = [\n",
    "        sub_id for sub_id, topics in submission_topics.items() \n",
    "        if topic in topics\n",
    "    ]\n",
    "    \n",
    "    # calculate average sentiment for this topic.\n",
    "    sentiments = [submission_metadata[sub_id]['sentiment'] for sub_id in relevant_submissions if submission_metadata[sub_id]['sentiment'] != 0]\n",
    "    avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0\n",
    "    \n",
    "    # get timestamps for temporal analysis.\n",
    "    timestamps = [submission_metadata[sub_id]['timestamp'] for sub_id in relevant_submissions \n",
    "                 if submission_metadata[sub_id]['timestamp'] is not None]\n",
    "    \n",
    "    # store topic data.\n",
    "    topic_analysis[topic] = {\n",
    "        'count': len(relevant_submissions),\n",
    "        'avg_sentiment': avg_sentiment,\n",
    "        'submissions': relevant_submissions,\n",
    "        'earliest_mention': min(timestamps) if timestamps else None,\n",
    "        'latest_mention': max(timestamps) if timestamps else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# create a DataFrame for easier analysis and graphing.\n",
    "rows = []\n",
    "for submission_id, metadata in submission_metadata.items():\n",
    "    for topic in metadata['topics']:\n",
    "        rows.append({\n",
    "            'submission_id': submission_id,\n",
    "            'date': metadata['date'],\n",
    "            'time': metadata['time'],\n",
    "            'timestamp': metadata['timestamp'],\n",
    "            'sentiment': metadata['sentiment'],\n",
    "            'num_comments': metadata['num_comments'],\n",
    "            'topic': topic\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# save the DataFrame for later visualisation.\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "# save the full analysis results to JSON.\n",
    "results = {\n",
    "    'submission_metadata': submission_metadata,\n",
    "    'submission_topics': submission_topics,\n",
    "    'topic_analysis': {k: v for k, v in topic_analysis.items() if k},\n",
    "    'all_topics': list(all_topics)\n",
    "}\n",
    "\n",
    "print(f\"Found {len(all_topics)} unique topics across {len(submission_texts)} submissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Most Common Topics Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. most common topics\n",
    "topic_counts = Counter([topic for topics_list in submission_topics.values() for topic in topics_list])\n",
    "top_topics = topic_counts.most_common(15)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=[topic for topic, _ in top_topics], y=[count for _, count in top_topics])\n",
    "plt.title('Most Common Topics')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_topics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Most Common Topics Sentiment Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. sentiment distribution by top topics\n",
    "top_topic_names = [topic for topic, _ in top_topics[:15]]\n",
    "topic_sentiments = df[df['topic'].isin(top_topic_names)]\n",
    "# get topics in order of frequency\n",
    "ordered_topics = [topic for topic, _ in top_topics[:15]]\n",
    "\n",
    "# Use this order in the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='topic', y='sentiment', data=topic_sentiments, order=ordered_topics)\n",
    "plt.title('Sentiment Distribution by Topic')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_sentiments.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Topics Highest and Lowest Rated Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Print min and max sentiment submission IDs for top 5 topics\n",
    "print(\"\\n--- Min/Max Sentiment Submission IDs for Top 5 Topics ---\")\n",
    "for topic, count in top_topics[:25]:\n",
    "    print(f\"\\nTopic: {topic} (Count: {count})\")\n",
    "    # Get relevant submissions and their sentiment scores\n",
    "    topic_submissions = []\n",
    "    for sub_id in topic_analysis[topic]['submissions']:\n",
    "        sentiment = submission_metadata[sub_id]['sentiment']\n",
    "        date = submission_metadata[sub_id]['date']\n",
    "        topic_submissions.append((sub_id, sentiment, date))\n",
    "    \n",
    "    # Skip if no submissions with valid sentiment scores\n",
    "    if not topic_submissions:\n",
    "        print(\" No submissions with sentiment scores found\")\n",
    "        continue\n",
    "    \n",
    "    # Find min and max sentiment submissions\n",
    "    min_submission = min(topic_submissions, key=lambda x: x[1])\n",
    "    max_submission = max(topic_submissions, key=lambda x: x[1])\n",
    "    \n",
    "    # Print min sentiment submission\n",
    "    print(f\" Minimum Sentiment:\")\n",
    "    print(f\" Submission ID: {min_submission[0]}\")\n",
    "    print(f\" Sentiment Score: {min_submission[1]:.4f}\")\n",
    "    print(f\" Date: {min_submission[2]}\")\n",
    "    \n",
    "    # Print max sentiment submission\n",
    "    print(f\" Maximum Sentiment:\")\n",
    "    print(f\" Submission ID: {max_submission[0]}\")\n",
    "    print(f\" Sentiment Score: {max_submission[1]:.4f}\")\n",
    "    print(f\" Date: {max_submission[2]}\")\n",
    "    \n",
    "    # Create sentiment trend graph\n",
    "    # Instead of filtering the DataFrame by topic (which doesn't exist),\n",
    "    # Create a new DataFrame for this specific topic using the submission data\n",
    "    \n",
    "    # Get all submission IDs for this topic with their timestamps and sentiment scores\n",
    "    topic_data = []\n",
    "    for sub_id in topic_analysis[topic]['submissions']:\n",
    "        if sub_id in submission_metadata:\n",
    "            timestamp = submission_metadata[sub_id].get('timestamp')\n",
    "            sentiment = submission_metadata[sub_id].get('sentiment')\n",
    "            if timestamp is not None and sentiment is not None:\n",
    "                topic_data.append({\n",
    "                    'submission_id': sub_id,\n",
    "                    'timestamp': timestamp,\n",
    "                    'sentiment': sentiment\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame just for this topic's submissions\n",
    "    topic_df = pd.DataFrame(topic_data)\n",
    "    \n",
    "    # If we have data, create the trend graph\n",
    "    if not topic_df.empty and 'timestamp' in topic_df.columns:\n",
    "        topic_df = topic_df.sort_values('timestamp')\n",
    "        topic_df['datetime'] = pd.to_datetime(topic_df['timestamp'], unit='s')\n",
    "        \n",
    "        # Continue with plotting or other analysis\n",
    "        # (Add your plotting code here)\n",
    "        print(f\" Sentiment trend analysis ready for {len(topic_df)} submissions\")\n",
    "    else:\n",
    "        print(f\" Not enough data for sentiment trend analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Word Clouds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty dictionary for word cloud\n",
    "top_topics = topic_counts.most_common(150)\n",
    "wordcloud_topics = {}\n",
    "\n",
    "# Your topic data using frequency for size and sentiment for color\n",
    "for topic, count in top_topics:\n",
    "    # Use the count (frequency) as the value for sizing\n",
    "    wordcloud_topics[topic] = count  # This will make size based on frequency\n",
    "\n",
    "# Get sentiment values for coloring\n",
    "sentiment_dict = {topic: topic_analysis[topic]['avg_sentiment'] for topic, _ in top_topics}\n",
    "min_sentiment = min(sentiment_dict.values())\n",
    "max_sentiment = max(sentiment_dict.values())\n",
    "\n",
    "# Color function based on sentiment - red to yellow to green\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    sentiment = sentiment_dict.get(word, 0)\n",
    "    # Normalize sentiment to [0,1]\n",
    "    normalized = (sentiment - min_sentiment) / (max_sentiment - min_sentiment) if max_sentiment > min_sentiment else 0.5\n",
    "    \n",
    "    # Convert to a color (red to yellow to green)\n",
    "    if normalized < 0.5:\n",
    "        # Red to yellow (bad to neutral)\n",
    "        # Increase green component as we move toward middle\n",
    "        return f\"rgb(255, {int(255 * normalized * 2)}, 0)\"\n",
    "    else:\n",
    "        # Yellow to green (neutral to good)\n",
    "        # Decrease red component as we move toward good\n",
    "        return f\"rgb({int(255 * (1 - (normalized - 0.5) * 2))}, 255, 0)\"\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    background_color='white',\n",
    "    color_func=color_func,  # Custom color function based on sentiment\n",
    "    prefer_horizontal=0.9,  # Allow some vertical words for better packing\n",
    "    max_words=100,\n",
    "    relative_scaling=0.6,  # Balance frequency vs. ranking\n",
    "    min_font_size=8,\n",
    "    max_font_size=80,\n",
    "    random_state=42  # Fixed seed for reproducibility\n",
    ").generate_from_frequencies(wordcloud_topics)\n",
    "\n",
    "# Create a figure for the word cloud\n",
    "plt.figure(figsize=(12, 8), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# Add a title and explain the color scheme\n",
    "plt.title(\"Topic Frequency with Sentiment Color\", fontsize=16, pad=20)\n",
    "\n",
    "# Add a small color legend\n",
    "ax = plt.gca()\n",
    "ax.text(0.01, 0.01, \"Red = Negative Sentiment\", transform=ax.transAxes, \n",
    "        fontsize=10, color='darkred', ha='left')\n",
    "ax.text(0.5, 0.01, \"Yellow = Neutral Sentiment\", transform=ax.transAxes, \n",
    "        fontsize=10, color='goldenrod', ha='center')\n",
    "ax.text(0.99, 0.01, \"Green = Positive Sentiment\", transform=ax.transAxes, \n",
    "        fontsize=10, color='darkgreen', ha='right')\n",
    "\n",
    "# Save the generated image as a PNG file\n",
    "wordcloud.to_file(\"topic_frequency_wordcloud.png\")\n",
    "print(\"Word cloud saved as 'topic_frequency_wordcloud.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Words per LDA Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(lda_model, feature_names, n_top_words=10, n_topics=15):\n",
    "    # plot the top words for each topic from the LDA model.\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(20, 12), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        if topic_idx >= n_topics:\n",
    "            break\n",
    "            \n",
    "        top_features_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_idx]\n",
    "        weights = topic[top_features_idx]\n",
    "        \n",
    "        # normalise weights for better visualisation.\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx+1}', fontdict={'fontsize': 12})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle('Top Words per LDA Topic', fontsize=16)\n",
    "        \n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.30, hspace=0.35)\n",
    "    plt.show()\n",
    "    \n",
    "num_topics = lda_model.n_components\n",
    "    \n",
    "# plot top words for each topic.\n",
    "plot_top_words(lda_model, feature_names, n_topics=num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clusters for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def create_topic_visualisation(top_terms_by_topic):\n",
    "    \"\"\"\n",
    "    Create a visually appealing topic correlation network visualisation.\n",
    "    \n",
    "    :param top_terms_by_topic: Dictionary of topics and their top terms\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # create a graph.\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # generate colour palette dynamically.\n",
    "    colours = sns.color_palette(\"husl\", len(top_terms_by_topic))\n",
    "    \n",
    "    # build graph with term connections within topics.\n",
    "    for topic_idx, terms in top_terms_by_topic.items():\n",
    "        for i, term1 in enumerate(terms):\n",
    "            for term2 in terms[i+1:]:\n",
    "                G.add_edge(term1, term2, topic=topic_idx)\n",
    "    \n",
    "    # compute node positions.\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    \n",
    "    # visualise topic areas.\n",
    "    for idx, (topic_idx, terms) in enumerate(top_terms_by_topic.items()):\n",
    "        topic_pos = {term: pos[term] for term in terms if term in pos}\n",
    "        \n",
    "        if topic_pos:\n",
    "            # calculate topic area.\n",
    "            centres = np.array(list(topic_pos.values()))\n",
    "            centre = centres.mean(axis=0)\n",
    "            radius = np.max(np.linalg.norm(centres - centre, axis=1)) * 1.3\n",
    "            \n",
    "            # draw topic circle.\n",
    "            circle = plt.Circle(centre, radius, color=colours[idx], alpha=0.1)\n",
    "            plt.gca().add_patch(circle)\n",
    "            \n",
    "            # add topic label.\n",
    "            plt.text(centre[0], centre[1], f\"Topic {topic_idx}\", \n",
    "                     fontsize=12, fontweight='bold', \n",
    "                     ha='center', va='center',\n",
    "                     bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))\n",
    "    \n",
    "    # draw term connections.\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        topic_idx = data['topic']\n",
    "        colour_idx = list(top_terms_by_topic.keys()).index(topic_idx)\n",
    "        plt.plot([pos[u][0], pos[v][0]], \n",
    "                 [pos[u][1], pos[v][1]], \n",
    "                 color=colours[colour_idx], \n",
    "                 alpha=0.3, \n",
    "                 linewidth=1)\n",
    "    \n",
    "    # compute term frequencies.\n",
    "    term_counts = {}\n",
    "    for terms in top_terms_by_topic.values():\n",
    "        for term in terms:\n",
    "            term_counts[term] = term_counts.get(term, 0) + 1\n",
    "    \n",
    "    # draw nodes and labels.\n",
    "    for term, (x, y) in pos.items():\n",
    "        # find the topic for this term.\n",
    "        topic_idx = next((idx for idx, terms in top_terms_by_topic.items() if term in terms), None)\n",
    "        \n",
    "        if topic_idx is not None:\n",
    "            # determine node size and colour.\n",
    "            node_size = 300 + 200 * term_counts.get(term, 1)\n",
    "            colour_idx = list(top_terms_by_topic.keys()).index(topic_idx)\n",
    "            \n",
    "            # plot node.\n",
    "            plt.scatter(x, y, \n",
    "                        s=node_size, \n",
    "                        color=colours[colour_idx], \n",
    "                        alpha=0.7, \n",
    "                        edgecolors='black', \n",
    "                        linewidth=1)\n",
    "            \n",
    "            # add term label.\n",
    "            plt.text(x, y, term, \n",
    "                     fontsize=8, \n",
    "                     ha='center', \n",
    "                     va='center', \n",
    "                     fontweight='bold')\n",
    "    \n",
    "    # finalise visualisation.\n",
    "    plt.title('Topic Terms Correlation Network', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # create legend.\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', \n",
    "                   label=f'Topic {topic_idx}: {\", \".join(terms[:3])}...', \n",
    "                   markerfacecolor=colours[list(top_terms_by_topic.keys()).index(topic_idx)], \n",
    "                   markersize=10)\n",
    "        for topic_idx, terms in top_terms_by_topic.items()\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, \n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, -0.05),\n",
    "               fancybox=True, \n",
    "               shadow=True, \n",
    "               ncol=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def prepare_and_visualise_topics(vectoriser, lda_model):\n",
    "    \"\"\"\n",
    "    Prepare topic terms and create visualisation.\n",
    "    \n",
    "    :param vectoriser: Fitted CountVectoriser or TfidfVectoriser\n",
    "    :param lda_model: Fitted LDA model\n",
    "    \"\"\"\n",
    "    # extract feature names.\n",
    "    feature_names = vectoriser.get_feature_names_out()\n",
    "    \n",
    "    # prepare top terms for each topic.\n",
    "    top_terms_by_topic = {}\n",
    "    for topic_idx, topic_vec in enumerate(lda_model.components_):\n",
    "        # get indices of top terms for this topic.\n",
    "        top_term_indices = topic_vec.argsort()[:-10 - 1:-1]\n",
    "        \n",
    "        # extract top terms.\n",
    "        top_terms = [feature_names[i] for i in top_term_indices]\n",
    "        top_terms_by_topic[topic_idx] = top_terms\n",
    "    \n",
    "    # create visualisation.\n",
    "    create_topic_visualisation(top_terms_by_topic)\n",
    "\n",
    "prepare_and_visualise_topics(count_vectorizer, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "def create_dataframe_from_metadata(submission_metadata):\n",
    "    \"\"\"\n",
    "    convert submission_metadata dictionary to a dataframe.\n",
    "    \n",
    "    :param submission_metadata: dictionary containing submission metadata\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for submission_id, metadata in submission_metadata.items():\n",
    "        # ensure topics is a list.\n",
    "        topics = metadata.get('topics', [])\n",
    "        if not isinstance(topics, list):\n",
    "            topics = [topics]\n",
    "        \n",
    "        for topic in topics:\n",
    "            # ensure topic is a string and convert to lowercase.\n",
    "            topic = str(topic).lower().strip()\n",
    "            \n",
    "            # skip empty topics.\n",
    "            if not topic:\n",
    "                continue\n",
    "            \n",
    "            rows.append({\n",
    "                'submission_id': submission_id,\n",
    "                'date': pd.to_datetime(metadata['timestamp'], unit='s'),\n",
    "                'timestamp': pd.to_datetime(metadata['timestamp'], unit='s'),\n",
    "                'sentiment': metadata['sentiment'],\n",
    "                'num_comments': metadata['num_comments'],\n",
    "                'topic': topic\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def analyze_topics_sentiment(df, topics=None):\n",
    "    \"\"\"\n",
    "    analyse sentiment for topics with comprehensive printing.\n",
    "    \n",
    "    :param df: dataframe with submission data\n",
    "    :param topics: list of topics to analyse (optional)\n",
    "    \"\"\"\n",
    "    # if no topics specified, use all unique topics.\n",
    "    if topics is None:\n",
    "        topics = df['topic'].unique().tolist()\n",
    "    else:\n",
    "        # ensure topics are lowercase and stripped.\n",
    "        topics = [str(topic).lower().strip() for topic in topics]\n",
    "    \n",
    "    # filter for specified topics.\n",
    "    filtered_df = df[df['topic'].isin(topics)]\n",
    "    \n",
    "    # print overall data insights.\n",
    "    print(\"Data Analysis Overview:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total number of submissions: {len(df)}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"Total days in dataset: {(df['date'].max() - df['date'].min()).days + 1} days\")\n",
    "    print(\"\\nTopic Distribution:\")\n",
    "    topic_counts = filtered_df['topic'].value_counts()\n",
    "    print(topic_counts)\n",
    "    \n",
    "    print(\"\\nSentiment Statistics by Topic:\")\n",
    "    topic_sentiment_stats = filtered_df.groupby('topic')['sentiment'].agg([\n",
    "        'count', \n",
    "        'mean', \n",
    "        'median', \n",
    "        'min', \n",
    "        'max', \n",
    "        'std'\n",
    "    ])\n",
    "    print(topic_sentiment_stats)\n",
    "    \n",
    "    # plotting.\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # group by date and topic, calculate daily average sentiment.\n",
    "    daily_sentiment = filtered_df.groupby([\n",
    "        pd.Grouper(key='date', freq='D'), \n",
    "        'topic'\n",
    "    ])['sentiment'].mean().reset_index()\n",
    "    \n",
    "    # pivot for plotting.\n",
    "    pivot_df = daily_sentiment.pivot(index='date', columns='topic', values='sentiment')\n",
    "    \n",
    "    # Calculate and print correlation between topics\n",
    "    if len(topics) >= 2:\n",
    "        print(\"\\nCorrelation between Topics:\")\n",
    "        print(\"-\" * 30)\n",
    "        correlation_matrix = pivot_df.corr(method='pearson')\n",
    "        print(correlation_matrix)\n",
    "        \n",
    "        # Print a more descriptive interpretation of the correlation\n",
    "        for i in range(len(topics)):\n",
    "            for j in range(i+1, len(topics)):\n",
    "                topic1 = topics[i]\n",
    "                topic2 = topics[j]\n",
    "                if topic1 in correlation_matrix.index and topic2 in correlation_matrix.columns:\n",
    "                    corr_value = correlation_matrix.loc[topic1, topic2]\n",
    "                    print(f\"\\nCorrelation between '{topic1}' and '{topic2}' sentiment: {corr_value:.4f}\")\n",
    "                    \n",
    "                    # Interpret the correlation strength\n",
    "                    if abs(corr_value) < 0.3:\n",
    "                        strength = \"weak\"\n",
    "                    elif abs(corr_value) < 0.7:\n",
    "                        strength = \"moderate\"\n",
    "                    else:\n",
    "                        strength = \"strong\"\n",
    "                        \n",
    "                    direction = \"positive\" if corr_value > 0 else \"negative\"\n",
    "                    \n",
    "                    if abs(corr_value) < 0.1:\n",
    "                        print(f\"There is virtually no correlation between these topics' sentiment trends.\")\n",
    "                    else:\n",
    "                        print(f\"This indicates a {strength} {direction} correlation - when sentiment for '{topic1}' goes \" + \n",
    "                              f\"{'up' if corr_value > 0 else 'down'}, sentiment for '{topic2}' tends to go \" + \n",
    "                              f\"{'up' if corr_value > 0 else 'down'} as well.\")\n",
    "    \n",
    "    # plot each topic.\n",
    "    for topic in pivot_df.columns:\n",
    "        plt.plot(pivot_df.index, pivot_df[topic], label=topic, marker='o')\n",
    "    \n",
    "    plt.title('Daily Average Sentiment by Topic', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Average Sentiment Score', fontsize=12)\n",
    "    plt.legend(title='Topics')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # display plot in jupyter notebook.\n",
    "    plt.show()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# debugging function to print out all unique topics.\n",
    "def print_all_topics(submission_metadata):\n",
    "    \"\"\"\n",
    "    print all unique topics in the submission metadata.\n",
    "    \"\"\"\n",
    "    all_topics = set()\n",
    "    for metadata in submission_metadata.values():\n",
    "        topics = metadata.get('topics', [])\n",
    "        if not isinstance(topics, list):\n",
    "            topics = [topics]\n",
    "        all_topics.update(str(topic).lower().strip() for topic in topics if topic)\n",
    "    \n",
    "    print(\"All unique topics:\")\n",
    "    print(sorted(all_topics))\n",
    "\n",
    "print_all_topics(submission_metadata)\n",
    "df = create_dataframe_from_metadata(submission_metadata)\n",
    "result = analyze_topics_sentiment(df, topics=['trump', 'buy'])\n",
    "result = analyze_topics_sentiment(df, topics=['trump', 'sell'])\n",
    "result = analyze_topics_sentiment(df, topics=['tariff', 'trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "\n",
    "# function to create dataframe from metadata.\n",
    "def create_dataframe_from_metadata(submission_metadata):\n",
    "\n",
    "    rows = []\n",
    "    for submission_id, metadata in submission_metadata.items():\n",
    "        # Ensure topics is a list\n",
    "        topics = metadata.get('topics', [])\n",
    "        if not isinstance(topics, list):\n",
    "            topics = [topics]\n",
    "        \n",
    "        for topic in topics:\n",
    "            # Ensure topic is a string and convert to lowercase\n",
    "            topic = str(topic).lower().strip()\n",
    "            \n",
    "            # Skip empty topics\n",
    "            if not topic:\n",
    "                continue\n",
    "            \n",
    "            rows.append({\n",
    "                'submission_id': submission_id,\n",
    "                'date': pd.to_datetime(metadata['timestamp'], unit='s'),\n",
    "                'timestamp': pd.to_datetime(metadata['timestamp'], unit='s'),\n",
    "                'sentiment': metadata['sentiment'],\n",
    "                'num_comments': metadata['num_comments'],\n",
    "                'topic': topic\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Function to fetch real S&P 500 data using yfinance\n",
    "def fetch_sp500_data(start_date, end_date):\n",
    "    # Add a buffer to make sure we get data for the whole period\n",
    "    fetch_start = start_date - timedelta(days=5)\n",
    "    fetch_end = end_date + timedelta(days=5)\n",
    "    \n",
    "    try:\n",
    "        # Fetch S&P 500 data using the ^GSPC ticker\n",
    "        print(f\"Fetching S&P 500 data from {fetch_start.date()} to {fetch_end.date()}\")\n",
    "        sp500_data = yf.download('^GSPC', start=fetch_start, end=fetch_end)\n",
    "        \n",
    "        if len(sp500_data) == 0:\n",
    "            raise ValueError(\"No data returned from Yahoo Finance\")\n",
    "            \n",
    "        print(f\"Successfully fetched {len(sp500_data)} days of S&P 500 data\")\n",
    "        \n",
    "        # Reset index to make 'Date' a column\n",
    "        sp500_data = sp500_data.reset_index()\n",
    "        \n",
    "        # Rename columns for consistency\n",
    "        sp500_data = sp500_data.rename(columns={'Date': 'date', 'Close': 'close'})\n",
    "        \n",
    "        # Ensure close is numeric\n",
    "        sp500_data['close'] = pd.to_numeric(sp500_data['close'], errors='coerce')\n",
    "        \n",
    "        # Print the first few rows to debug\n",
    "        print(\"\\nS&P 500 Data Sample:\")\n",
    "        print(sp500_data.head())\n",
    "        \n",
    "        return sp500_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching S&P 500 data: {e}\")\n",
    "        print(\"Falling back to simulated data...\")\n",
    "        \n",
    "        # Create simulated data as fallback\n",
    "        date_range = pd.date_range(start=fetch_start, end=fetch_end, freq='B')  # Business days\n",
    "        np.random.seed(42)\n",
    "        close_prices = 5000 + np.cumsum(np.random.normal(0, 10, len(date_range)))\n",
    "        \n",
    "        sp500_data = pd.DataFrame({\n",
    "            'date': date_range,\n",
    "            'close': close_prices\n",
    "        })\n",
    "        \n",
    "        return sp500_data\n",
    "\n",
    "# Function to plot sentiment against real S&P 500 data\n",
    "def plot_real_sentiment_sp500(df, topic='trump'):\n",
    "    \"\"\"\n",
    "    Plot sentiment data against real S&P 500 data\n",
    "    \n",
    "    :param df: DataFrame with submission data\n",
    "    :param topic: Topic to analyze\n",
    "    \"\"\"\n",
    "    # Filter for the specific topic\n",
    "    topic_df = df[df['topic'] == topic].copy()\n",
    "    \n",
    "    print(f\"Number of '{topic}' entries: {len(topic_df)}\")\n",
    "    \n",
    "    if len(topic_df) == 0:\n",
    "        print(f\"No data found for topic '{topic}'\")\n",
    "        return None\n",
    "    \n",
    "    # Print the first few rows\n",
    "    print(\"\\nSample of filtered data:\")\n",
    "    print(topic_df.head())\n",
    "    \n",
    "    # Print date range\n",
    "    print(f\"\\nDate range: {topic_df['date'].min()} to {topic_df['date'].max()}\")\n",
    "    print(f\"Number of unique dates: {topic_df['date'].dt.date.nunique()}\")\n",
    "    \n",
    "    # Sort by date\n",
    "    topic_df = topic_df.sort_values('date')\n",
    "    \n",
    "    # Get date range for S&P 500 data\n",
    "    start_date = topic_df['date'].min() - timedelta(days=1)\n",
    "    end_date = topic_df['date'].max() + timedelta(days=1)\n",
    "    \n",
    "    # Fetch real S&P 500 data\n",
    "    sp500 = fetch_sp500_data(start_date, end_date)\n",
    "    \n",
    "    # Aggregate sentiment by day (mean)\n",
    "    daily_sentiment = topic_df.groupby(topic_df['date'].dt.date)['sentiment'].mean().reset_index()\n",
    "    daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
    "    \n",
    "    print(f\"\\nDaily sentiment aggregation (first few rows):\")\n",
    "    print(daily_sentiment.head())\n",
    "    \n",
    "    # Create a date range for every day in the period\n",
    "    full_date_range = pd.date_range(start=daily_sentiment['date'].min(), end=daily_sentiment['date'].max(), freq='D')\n",
    "    \n",
    "    # Create a template DataFrame with all dates\n",
    "    template_df = pd.DataFrame({'date': full_date_range})\n",
    "    \n",
    "    # Merge with the daily sentiment\n",
    "    merged_sentiment = pd.merge(template_df, daily_sentiment, on='date', how='left')\n",
    "    \n",
    "    print(\"\\nMerged sentiment with full date range (first few rows):\")\n",
    "    print(merged_sentiment.head())\n",
    "    \n",
    "    # Print raw statistics before normalization\n",
    "    print(\"\\nRaw Statistics (before normalization):\")\n",
    "    print(f\"Original sentiment mean: {merged_sentiment['sentiment'].dropna().mean():.6f}\")\n",
    "    print(f\"Original sentiment std: {merged_sentiment['sentiment'].dropna().std():.6f}\")\n",
    "    print(f\"Original sentiment min: {merged_sentiment['sentiment'].dropna().min():.6f}\")\n",
    "    print(f\"Original sentiment max: {merged_sentiment['sentiment'].dropna().max():.6f}\")\n",
    "    \n",
    "    # Make sure sp500 close is properly extracted as a numeric series\n",
    "    sp500_close = sp500['close'].astype(float)\n",
    "    \n",
    "    print(f\"Original S&P 500 mean: {sp500_close.mean():.6f}\")\n",
    "    print(f\"Original S&P 500 std: {sp500_close.std():.6f}\")\n",
    "    print(f\"Original S&P 500 min: {sp500_close.min():.6f}\")\n",
    "    print(f\"Original S&P 500 max: {sp500_close.max():.6f}\")\n",
    "    \n",
    "    # Normalize sentiment\n",
    "    sentiment_mean = merged_sentiment['sentiment'].dropna().mean()\n",
    "    sentiment_std = merged_sentiment['sentiment'].dropna().std()\n",
    "    merged_sentiment['normalized_sentiment'] = (merged_sentiment['sentiment'] - sentiment_mean) / sentiment_std\n",
    "    \n",
    "    # Normalize S&P 500 - first create a consistent daily series\n",
    "    # Convert date to datetime if it's not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(sp500['date']):\n",
    "        sp500['date'] = pd.to_datetime(sp500['date'])\n",
    "    \n",
    "    # Set date as index for easy resampling\n",
    "    sp500 = sp500.set_index('date')\n",
    "    \n",
    "    # Ensure we have data for every day by resampling\n",
    "    # Forward fill for weekends/holidays, then backward fill any remaining NaNs\n",
    "    daily_sp500 = sp500['close'].resample('D').ffill().bfill()\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    daily_sp500 = pd.DataFrame({'date': daily_sp500.index, 'close': daily_sp500.values})\n",
    "    \n",
    "    # Normalize S&P 500\n",
    "    sp500_mean = daily_sp500['close'].mean()\n",
    "    sp500_std = daily_sp500['close'].std()\n",
    "    daily_sp500['normalized_close'] = (daily_sp500['close'] - sp500_mean) / sp500_std\n",
    "    \n",
    "    # Validation of normalization\n",
    "    print(\"\\nS&P 500 Normalization Validation:\")\n",
    "    print(f\"Original S&P 500 mean: {daily_sp500['close'].mean():.6f}\")\n",
    "    print(f\"Original S&P 500 std: {daily_sp500['close'].std():.6f}\")\n",
    "    print(f\"Normalized S&P 500 mean: {daily_sp500['normalized_close'].mean():.6f}\")\n",
    "    print(f\"Normalized S&P 500 std: {daily_sp500['normalized_close'].std():.6f}\")\n",
    "    \n",
    "    # Check range\n",
    "    print(f\"Original S&P 500 min: {daily_sp500['close'].min():.6f}\")\n",
    "    print(f\"Original S&P 500 max: {daily_sp500['close'].max():.6f}\")\n",
    "    print(f\"Normalized S&P 500 min: {daily_sp500['normalized_close'].min():.6f}\")\n",
    "    print(f\"Normalized S&P 500 max: {daily_sp500['normalized_close'].max():.6f}\")\n",
    "    \n",
    "    print(\"\\nSentiment Normalization Validation:\")\n",
    "    print(f\"Original sentiment mean: {merged_sentiment['sentiment'].dropna().mean():.6f}\")\n",
    "    print(f\"Original sentiment std: {merged_sentiment['sentiment'].dropna().std():.6f}\")\n",
    "    print(f\"Normalized sentiment mean: {merged_sentiment['normalized_sentiment'].dropna().mean():.6f}\")\n",
    "    print(f\"Normalized sentiment std: {merged_sentiment['normalized_sentiment'].dropna().std():.6f}\")\n",
    "    \n",
    "    # Check range\n",
    "    print(f\"Original sentiment min: {merged_sentiment['sentiment'].dropna().min():.6f}\")\n",
    "    print(f\"Original sentiment max: {merged_sentiment['sentiment'].dropna().max():.6f}\")\n",
    "    print(f\"Normalized sentiment min: {merged_sentiment['normalized_sentiment'].dropna().min():.6f}\")\n",
    "    print(f\"Normalized sentiment max: {merged_sentiment['normalized_sentiment'].dropna().max():.6f}\")\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Filter S&P 500 to the date range we need\n",
    "    plot_start = merged_sentiment['date'].min()\n",
    "    plot_end = merged_sentiment['date'].max()\n",
    "    plotting_sp500 = daily_sp500[(daily_sp500['date'] >= plot_start) & (daily_sp500['date'] <= plot_end)]\n",
    "    \n",
    "    # Plot S&P 500\n",
    "    plt.plot(\n",
    "        plotting_sp500['date'], \n",
    "        plotting_sp500['normalized_close'], \n",
    "        label='S&P 500', \n",
    "        color='black', \n",
    "        linewidth=2\n",
    "    )\n",
    "    \n",
    "    # Plot sentiment data\n",
    "    valid_mask = ~merged_sentiment['sentiment'].isna()\n",
    "    \n",
    "    if valid_mask.sum() > 0:  # Check if we have any valid data points\n",
    "        # First, draw a line for all points (including interpolated ones)\n",
    "        # Use more modern methods instead of fillna with method\n",
    "        interpolated_sentiment = merged_sentiment['normalized_sentiment'].copy()\n",
    "        interpolated_sentiment = interpolated_sentiment.interpolate(method='linear')\n",
    "        interpolated_sentiment = interpolated_sentiment.bfill().ffill()\n",
    "        \n",
    "        sentiment_line = plt.plot(\n",
    "            merged_sentiment['date'], \n",
    "            interpolated_sentiment, \n",
    "            color='blue', \n",
    "            alpha=0.5,\n",
    "            linewidth=1.5,\n",
    "            label=f'{topic} sentiment (interpolated)'\n",
    "        )\n",
    "        \n",
    "        # Then, plot markers for actual data points\n",
    "        sentiment_points = plt.plot(\n",
    "            merged_sentiment.loc[valid_mask, 'date'], \n",
    "            merged_sentiment.loc[valid_mask, 'normalized_sentiment'], \n",
    "            'o', \n",
    "            color='blue', \n",
    "            markersize=6, \n",
    "            label=f'{topic} sentiment (actual data)'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning: No valid sentiment data points found after aggregation\")\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f'Normalized Values: Real S&P 500 vs. {topic.capitalize()} Sentiment', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Normalized Value (Z-Score)', fontsize=12)\n",
    "    plt.legend(title='Data Series')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Format x-axis dates\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sp500_vs_sentiment.png')  # Save the figure to a file\n",
    "    plt.show()  # Display the figure (if in an environment that supports it)\n",
    "    \n",
    "    # Calculate correlation only if we have valid data\n",
    "    if valid_mask.sum() >= 2:\n",
    "        # Create a common date column for merging\n",
    "        merged_sentiment['date_only'] = merged_sentiment['date'].dt.date\n",
    "        daily_sp500['date_only'] = pd.to_datetime(daily_sp500['date']).dt.date\n",
    "        \n",
    "        # Merge the datasets\n",
    "        correlation_df = pd.merge(\n",
    "            merged_sentiment[['date_only', 'normalized_sentiment']], \n",
    "            daily_sp500[['date_only', 'normalized_close']], \n",
    "            on='date_only',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Filter out any NaN values\n",
    "        correlation_df = correlation_df.dropna()\n",
    "        \n",
    "        # Calculate correlation if we have enough data points\n",
    "        if len(correlation_df) >= 2:\n",
    "            correlation = correlation_df['normalized_sentiment'].corr(correlation_df['normalized_close'])\n",
    "            print(f\"\\nCorrelation between S&P 500 and {topic} sentiment: {correlation:.4f}\")\n",
    "    \n",
    "    return merged_sentiment, daily_sp500\n",
    "\n",
    "# Create DataFrame from submission metadata\n",
    "df = create_dataframe_from_metadata(submission_metadata)\n",
    "\n",
    "# Plot sentiment data against real S&P 500 data\n",
    "result = plot_real_sentiment_sp500(df, topic='trump')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
