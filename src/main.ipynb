{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<span style=\"color:red; font-size:2em; font-weight: bold\">Social Media Network Analytics - Assignment 1</span>\n",
    "\n",
    "## __Name: Harry Marshall__\n",
    "## __Student ID: s4015845__\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Collection__\n",
    "### Connect to Reddit Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reddit client that will gather all the data.\n",
    "from redditClient import redditClient\n",
    "\n",
    "# name of subreddit the data is collected from.\n",
    "subreddit_name= 'stocks'\n",
    "retrieved_result_limit = 5000\n",
    "\n",
    "# construct reddit client.\n",
    "# client = redditClient()\n",
    "\n",
    "# access the subreddit.\n",
    "# subreddit = client.subreddit(subreddit_name)\n",
    "# print subreddit title.\n",
    "# print(subreddit.title)\n",
    "# print subreddit descp.\n",
    "# print(subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount of Subreddit Submissions in a Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand the amount of reddit posts made in a month.\n",
    "# import praw\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # calculate date range.\n",
    "# end_date = datetime.utcnow()\n",
    "# start_date = end_date - timedelta(days=30)\n",
    "# start_timestamp = int(start_date.timestamp())\n",
    "\n",
    "# print(f\"Counting posts in r/{subreddit_name} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# # init post count.\n",
    "# post_count = 0\n",
    "\n",
    "# # use 'new' sorting to get posts.\n",
    "# for submission in subreddit.new(limit=None):\n",
    "#     # check if we're within our time range.\n",
    "#     if submission.created_utc < start_timestamp:\n",
    "#         # stop searching after a months worth of posts.\n",
    "#         break\n",
    "        \n",
    "#     post_count += 1\n",
    "    \n",
    "#     # print progress every 100 posts.\n",
    "#     if post_count % 100 == 0:\n",
    "#         print(f\"Counted {post_count} posts so far...\")\n",
    "\n",
    "# # print result.\n",
    "# print(f\"\\nRESULTS:\")\n",
    "# print(f\"Total posts in r/{subreddit_name} in the past 30 days: {post_count}\")\n",
    "# print(f\"Average posts per day: {post_count / 30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Structure Subreddit Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain months worth of reddit posts.\n",
    "# import json\n",
    "# from praw.models import MoreComments\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "\n",
    "# # create a json file name.\n",
    "# output_filename = f'../json/{subreddit_name}-submissions-raw.json'\n",
    "# submissions_json = []\n",
    "\n",
    "# # 750 posts have been made in a month.\n",
    "# for index, submission in enumerate(subreddit.new(limit=retrieved_result_limit)):\n",
    "#     if index % 100 == 0:\n",
    "#         print(f\"Collected {index + 1} submissions...\")\n",
    "\n",
    "#     submission_data = {\n",
    "#         \"submission_id\": submission.id,\n",
    "#         \"submission_date\": submission.created_utc,\n",
    "#         \"submission_title\": submission.title,\n",
    "#         \"submission_author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "#         \"submission_score\": submission.score,\n",
    "#         \"submission_upvote_ratio\": submission.upvote_ratio,\n",
    "#         \"submission_num_comments\": submission.num_comments,\n",
    "#         \"submission_text\": submission.selftext if submission.is_self else \"\",\n",
    "#         # Empty list to hold comments later if needed\n",
    "#         \"submission_comments\": [] \n",
    "#     }\n",
    "\n",
    "#     if submission.num_comments > 0:\n",
    "#         comments_list = [comment for comment in submission.comments.list() if not isinstance(comment, MoreComments)]\n",
    "\n",
    "#         for i, comment in enumerate(comments_list):\n",
    "\n",
    "#             if i >= 35:\n",
    "#                 break \n",
    "\n",
    "#             comment_data = {\n",
    "#                 \"comment_id\": comment.id,\n",
    "#                 \"comment_author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "#                 \"comment_score\": comment.score,\n",
    "#                 \"comment_text\": comment.body,\n",
    "#                 \"comment_num_replies\": len(comment.replies) if hasattr(comment, \"replies\") else 0\n",
    "#             }\n",
    "\n",
    "#             submission_data['submission_comments'].append(comment_data)\n",
    "\n",
    "#     submissions_json.append(submission_data)\n",
    "#     time.sleep(0.5)\n",
    "\n",
    "# # final header struct of json file.\n",
    "# final_data = {\n",
    "#     \"subreddit\": subreddit_name,\n",
    "#     \"collection_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#     \"total_posts\": len(submissions_json),\n",
    "#     \"posts\": submissions_json\n",
    "# }\n",
    "\n",
    "# # save to JSON file.\n",
    "# with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
    "#     print('json data saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Exploratory Data Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Unique and most Active Users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from praw.models import MoreComments\n",
    "# from collections import Counter\n",
    "\n",
    "# # specify which subreddit we are interested in.\n",
    "# subreddit = client.subreddit(subreddit_name)\n",
    "\n",
    "# l_post_author = []\n",
    "# l_comment_author = []\n",
    "\n",
    "# # obtain the hot submissions for this subreddit.\n",
    "# for submission in subreddit.hot(limit=750):\n",
    "#     # print title, but we can print other information of each submission as well.\n",
    "#     if submission.author is not None:\n",
    "#         l_post_author.append(submission.author.name)\n",
    "\n",
    "#     # check submission has comments.\n",
    "#     if submission.num_comments > 0:\n",
    "#         # remove all MoreComments class from list.\n",
    "#         comments_list = [comment for comment in submission.comments.list() if not isinstance(comment, MoreComments)]\n",
    "#         # stop after 35 comments.\n",
    "#         for i, comment in enumerate(comments_list):\n",
    "#             if i >= 35:\n",
    "#                 break\n",
    "\n",
    "#             if comment.author is not None:\n",
    "#                 l_comment_author.append(comment.author.name)\n",
    "\n",
    "# post_names_counter = Counter(l_post_author)\n",
    "# comment_names_counter = Counter(l_comment_author)\n",
    "\n",
    "# # print the results of post authors.\n",
    "# print(f\"Post submission authors: {post_names_counter}\")\n",
    "# print(f\"Number of Users: {len(post_names_counter)}\")\n",
    "\n",
    "# # print the results of comment authors.\n",
    "# print(f\"Post submission authors: {comment_names_counter}\")\n",
    "# print(f\"Number of Users: {len(comment_names_counter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Preprocessing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenisation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def process_text(text):\n",
    "\n",
    "    # Tweet tokeniser.\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    l_punct = list(string.punctuation)\n",
    "    # Add custom stopwords to the standard list\n",
    "    additional_stopwords = [\n",
    "        'would', 'could', 'also', 'like', 'one', 'go', 'see', 'think', 'make',\n",
    "        'get', 'take', 'look', 'know', 'want', 'say', 'said', 'even', 'still',\n",
    "        'much', 'many', 'back', 'good', 'new', 'this', 'that', 'these', 'those',\n",
    "        'has', 'had', 'was', 'were', 'been', 'being', 'have', 'any', 'some'\n",
    "    ]\n",
    "    stop_words = nltk.corpus.stopwords.words('english') + l_punct + ['via'] + additional_stopwords\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "    # Define stock ticker pattern.\n",
    "    ticker_pattern = r'\\$?[A-Z]{2,4}'\n",
    "    ticker_pattern_2 = r'[A-Z]{1,2}\\&[A-Z]{1,2}'\n",
    "\n",
    "    # Extract potential stock tickers before any processing.\n",
    "    potential_tickers = set(re.findall(ticker_pattern, text))\n",
    "    potential_tickers_2 = set(re.findall(ticker_pattern_2, text))\n",
    "    potential_tickers.update(potential_tickers_2)\n",
    "\n",
    "    # print known abbreviations. \n",
    "    print(potential_tickers)\n",
    "    \n",
    "    # convert to lower case, but AFTER extracting tickers.\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # get rid of all urls.\n",
    "    text_lower = re.sub(r'https?://\\S+|www\\.\\S+', '', text_lower)\n",
    "    \n",
    "    # filter out emoticons and other noisy characters.\n",
    "    text_lower = ''.join(char for char in text_lower if char.isalnum() or char.isspace() or char in string.punctuation)\n",
    "    \n",
    "    # tokenise text.\n",
    "    l_tokens = tokenizer.tokenize(text_lower)\n",
    "    # store original length of text.\n",
    "    original_word_count = len(l_tokens)\n",
    "    \n",
    "    # Strip whitespaces.\n",
    "    l_tokens = [token.strip() for token in l_tokens]\n",
    "    \n",
    "    # Remove very short tokens and stopwords early, but keep potential stock tickers\n",
    "    l_tokens = [token for token in l_tokens if \n",
    "                (token.upper() in potential_tickers) or  # Keep if it's a potential ticker\n",
    "                (len(token) > 2 and  \n",
    "                token not in stop_words and \n",
    "                not token.isdigit() and \n",
    "                token != \"'\" and token != \"''\" and\n",
    "                token != \"...\" and token != \"…\")]\n",
    "    \n",
    "    # now stem only the normal words, but leave stock tickers untouched\n",
    "    l_stemmed_tokens = set()\n",
    "    for tok in l_tokens:\n",
    "        if tok.upper() in potential_tickers:\n",
    "            # Keep tickers in uppercase to highlight them\n",
    "            l_stemmed_tokens.add(tok.upper())\n",
    "        else:\n",
    "            # Stem regular words\n",
    "            l_stemmed_tokens.add(stemmer.stem(tok))\n",
    "    \n",
    "    # calc the word count after tokenisation.\n",
    "    final_word_count = len(l_stemmed_tokens)\n",
    "    \n",
    "    # calculate dropped words.\n",
    "    words_dropped = original_word_count - final_word_count\n",
    "    return list(l_stemmed_tokens), words_dropped, original_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new JSON file with Preprocessed Text\n",
    "##### Using process_text() function on natural language.\n",
    "##### Printing names of stocks and financial acronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Enhanced regex patterns for stock symbols and market references - NO single letters\n",
    "patterns = [\n",
    "    # Standard stock ticker with $ prefix (STRICTLY 2-5 characters)\n",
    "    r'\\$([A-Z]{2,5})',\n",
    "    \n",
    "    # Stock tickers without $ prefix (STRICTLY 2-5 letters, surrounded by spaces or punctuation)\n",
    "    # Added more context to ensure it's likely a ticker\n",
    "    r'(?<=[^A-Za-z0-9])([A-Z]{2,5})(?=[^A-Za-z0-9])(?: (?:stock|shares|ticker|price|earnings|chart|gains|gains|calls|puts|options))',\n",
    "    \n",
    "    # Ticker specifically in \"buying/selling/holding X\" contexts\n",
    "    r'(?:buy|sell|hold|trade|short|long)(?:ing)?(?: some| a lot of| a few| many| the)? ([A-Z]{2,5})(?=[^A-Za-z0-9])',\n",
    "    \n",
    "    # Common market indices with various formats (made more specific)\n",
    "    r'(?:S&P|Standard(?: and|&) Poor(?:\\'s)?) ?(?:500|5O0|5OO)',\n",
    "    r'(?:Dow(?: Jones)?)(?: Industrial Average| Index)?',\n",
    "    r'NASDAQ(?: Composite| Index)?',\n",
    "    r'Russell(?: (?:1000|2000|3000))(?: Index)?',\n",
    "    r'NYSE(?:(?: Composite)? Index)?',\n",
    "    \n",
    "    # Major ETFs and common funds (specific list - explicitly 2+ characters only)\n",
    "    r'(?<=[^A-Za-z0-9])(SPY|QQQ|VOO|VTI|TQQQ|SQQQ|DIA|IWM|XLF|XLE|XLK|XLV|XLY|XLI|XLP|XLU|XLB|GLD|SLV|USO|VIX)(?=[^A-Za-z0-9])',\n",
    "    \n",
    "    # Stock mentions with stock keyword\n",
    "    r'([A-Z]{2,5}) (?:stock|shares|ticker)',\n",
    "    r'([A-Z]{2,5})\\'s (?:stock|share) price',\n",
    "    \n",
    "    # Company name followed by ticker in parentheses\n",
    "    r'([A-Za-z0-9\\s]+) \\(([A-Z]{2,5})\\)',\n",
    "    \n",
    "    # Ticker followed by company name in parentheses\n",
    "    r'([A-Z]{2,5}) \\(([A-Za-z0-9\\s]+)\\)',\n",
    "    \n",
    "    # Popular cryptocurrencies (specific list - all are 2+ characters)\n",
    "    r'(?<=[^A-Za-z0-9])(BTC|ETH|DOGE|XRP|SOL|ADA|DOT|SHIB|AVAX|LINK|MATIC|UNI)(?=[^A-Za-z0-9])',\n",
    "]\n",
    "\n",
    "# Comprehensive list of common acronyms to exclude \n",
    "common_acronyms = {\n",
    "    'AI', 'API', 'ATH', 'ATL', 'AMA', 'ASAP', 'CEO', 'CFO', 'CIO', 'COO', 'CTO', 'DIY', \n",
    "    'EPS', 'ESG', 'ETF', 'FAQ', 'FOMO', 'FUD', 'FYI', 'GDP', 'GG', 'HR', 'IMO', 'IMHO', \n",
    "    'IPO', 'IRS', 'IT', 'KPI', 'LMAO', 'LOL', 'MoM', 'NFT', 'NGMI', 'NSFW', 'NYC', 'OMG', \n",
    "    'OP', 'PE', 'POV', 'PR', 'PSA', 'QoQ', 'ROI', 'SEC', 'SaaS', 'SMH', 'SPAC', 'TBH', \n",
    "    'TIL', 'TLDR', 'USA', 'USD', 'UI', 'UX', 'VAT', 'VC', 'VIP', 'VS', 'WFH', 'YoY', 'YTD',\n",
    "    'DCA', 'EV', 'EU', 'UK', 'US'\n",
    "}\n",
    "\n",
    "# NO single-letter tickers allowed at all\n",
    "valid_single_letter_tickers = {}\n",
    "\n",
    "# List of known major stock tickers to validate against - REMOVED single letter tickers\n",
    "known_major_tickers = {\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'META', 'TSLA', 'NVDA', 'BRK.A', 'BRK.B', \n",
    "    'JPM', 'JNJ', 'PG', 'UNH', 'HD', 'BAC', 'XOM', 'CVX', 'MA', 'DIS', 'PFE', 'KO', \n",
    "    'AVGO', 'PEP', 'CSCO', 'TMO', 'COST', 'ABT', 'MRK', 'WMT', 'CRM', 'LLY', 'ACN', \n",
    "    'MCD', 'VZ', 'INTC', 'NFLX', 'AMD', 'ADBE', 'CMCSA', 'NKE', 'TXN', 'NEE', 'WFC', \n",
    "    'BMY', 'RTX', 'PM', 'UPS', 'QCOM', 'HON', 'MS', 'SBUX', 'AMAT', 'BA', 'GS', 'CAT',\n",
    "    'IBM', 'PYPL', 'BABA', 'GME', 'AMC', 'BB', 'NOK', 'PLTR', 'NIO', 'LCID', 'RIVN', 'COIN'\n",
    "}\n",
    "\n",
    "# Add ETFs and indices to the known symbols list\n",
    "known_etfs_and_indices = {\n",
    "    'SPY', 'QQQ', 'VOO', 'VTI', 'TQQQ', 'SQQQ', 'DIA', 'IWM', 'XLF', 'XLE', \n",
    "    'SPX', 'DJI', 'COMP', 'RUT', 'NYSE', 'VIX'\n",
    "}\n",
    "\n",
    "# Combine all known valid symbols\n",
    "known_valid_symbols = known_major_tickers.union(known_etfs_and_indices)\n",
    "\n",
    "def extract_stock_mentions(text):\n",
    "    mentions = []\n",
    "    \n",
    "    # Apply each pattern\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        # Handle tuples for patterns with multiple capture groups\n",
    "        for match in matches:\n",
    "            if isinstance(match, tuple):\n",
    "                for group in match:\n",
    "                    if group and len(group.strip()) > 0:\n",
    "                        mentions.append(group.strip())\n",
    "            else:\n",
    "                if match and len(match.strip()) > 0:\n",
    "                    mentions.append(match.strip())\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def filter_valid_symbols(symbol):\n",
    "    # Special cases that should always be allowed\n",
    "    special_cases = {\n",
    "        'S&P 500', 'S&P500', 'Standard and Poor 500', 'Standard & Poor 500',\n",
    "        'NASDAQ', 'Nasdaq', 'Nasdaq Composite', 'NASDAQ Composite',\n",
    "        'Dow Jones', 'DOW', 'Dow', 'Dow Jones Industrial Average',\n",
    "        'Russell 2000', 'NYSE'\n",
    "    }\n",
    "    \n",
    "    if symbol in special_cases:\n",
    "        return True\n",
    "    \n",
    "    # Remove $ prefix if present for further checks\n",
    "    clean_symbol = symbol[1:] if symbol.startswith('$') else symbol\n",
    "    \n",
    "    # ALWAYS reject ALL single-letter symbols with no exceptions\n",
    "    if len(clean_symbol) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Reject common acronyms that aren't stock symbols\n",
    "    if clean_symbol in common_acronyms and clean_symbol not in known_valid_symbols:\n",
    "        return False\n",
    "    \n",
    "    # Accept known valid tickers from our master list\n",
    "    if clean_symbol in known_valid_symbols:\n",
    "        return True\n",
    "    \n",
    "    # For other cases, require at least 2 characters\n",
    "    if len(clean_symbol) >= 2:\n",
    "        # Require all caps for standard tickers (not indices or special cases)\n",
    "        if clean_symbol.isupper() and clean_symbol.isalpha():\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def normalize_symbol(symbol):\n",
    "    # Handle special cases and standardize format\n",
    "    symbol_map = {\n",
    "        'S&P 500': 'SPX',\n",
    "        'S&P500': 'SPX',\n",
    "        'S&P': 'SPX',\n",
    "        'Standard and Poor 500': 'SPX',\n",
    "        'Standard & Poor 500': 'SPX',\n",
    "        'Dow Jones': 'DJI',\n",
    "        'Dow Jones Industrial Average': 'DJI',\n",
    "        'DOW': 'DJI',\n",
    "        'Dow': 'DJI',\n",
    "        'NASDAQ': 'COMP',\n",
    "        'NASDAQ Composite': 'COMP',\n",
    "        'Nasdaq': 'COMP',\n",
    "        'Nasdaq Composite': 'COMP',\n",
    "        'Russell 2000': 'RUT',\n",
    "        'NYSE': 'NYSE',\n",
    "        'Bitcoin': 'BTC',\n",
    "        'Ethereum': 'ETH',\n",
    "        'Dogecoin': 'DOGE'\n",
    "    }\n",
    "    \n",
    "    # Remove $ prefix if present\n",
    "    if symbol.startswith('$'):\n",
    "        symbol = symbol[1:]\n",
    "    \n",
    "    # Apply mapping for special cases\n",
    "    if symbol in symbol_map:\n",
    "        return symbol_map[symbol]\n",
    "    \n",
    "    # Return uppercase version for standard tickers\n",
    "    return symbol.upper()\n",
    "\n",
    "# Example of using these functions in the main processing loop:\n",
    "def process_text_for_stock_symbols(text, submission_id, stock_adj_list):\n",
    "    # Extract potential stock mentions\n",
    "    stock_symbols = extract_stock_mentions(text)\n",
    "    \n",
    "    # Filter and normalize symbols\n",
    "    for symbol in stock_symbols:\n",
    "        if filter_valid_symbols(symbol):\n",
    "            normalized_symbol = normalize_symbol(symbol)\n",
    "            # Double-check that normalized symbol isn't a single letter\n",
    "            if len(normalized_symbol) > 1:\n",
    "                stock_adj_list[normalized_symbol].append(submission_id)\n",
    "    \n",
    "    return stock_adj_list\n",
    "\n",
    "# Data structure to store stock symbols and their associated submission IDs\n",
    "stock_adj_list = defaultdict(list)\n",
    "\n",
    "# collected submission json file\n",
    "input_filename = f'../datasets/{subreddit_name}-submissions-raw.json'\n",
    "output_filename = f'../datasets/{subreddit_name}-submissions-token.json'\n",
    "\n",
    "# number of most frequent terms to display\n",
    "freq_num = 85\n",
    "\n",
    "# term freq counter\n",
    "term_freq_counter = Counter()\n",
    "\n",
    "# attributes in the json struct that need to be tokenised\n",
    "tokenised_submission = [\n",
    "    'submission_title',\n",
    "    'submission_text',\n",
    "]\n",
    "\n",
    "total_words_dropped = 0\n",
    "total_words_dataset = 0\n",
    "\n",
    "# open JSON file and process submissions\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    d_submissions = json.load(f)\n",
    "    for submission in d_submissions['posts']:\n",
    "        submission_id = submission.get('submission_id', '')\n",
    "        \n",
    "        # Process each text field in the submission\n",
    "        for var in tokenised_submission:\n",
    "            submission_text = submission.get(var, '')\n",
    "            \n",
    "            # Extract stock symbols before processing text\n",
    "            stock_symbols = extract_stock_mentions(submission_text)\n",
    "            \n",
    "            # Filter and normalize symbols\n",
    "            for symbol in stock_symbols:\n",
    "                if filter_valid_symbols(symbol):\n",
    "                    normalized_symbol = normalize_symbol(symbol)\n",
    "                    # Extra check to ensure no single-letter symbols get through\n",
    "                    if len(normalized_symbol) > 1:\n",
    "                        stock_adj_list[normalized_symbol].append(submission_id)\n",
    "            \n",
    "            # Process text normally\n",
    "            l_tokens, submission_words_dropped, submission_words = process_text(submission_text)\n",
    "            \n",
    "            # replace original text with tokenized version\n",
    "            submission[var] = ' '.join(l_tokens)\n",
    "            term_freq_counter.update(l_tokens)\n",
    "            total_words_dropped += submission_words_dropped\n",
    "            total_words_dataset += submission_words\n",
    "        \n",
    "        # Process each comment in the submission\n",
    "        for comment in submission['submission_comments']:\n",
    "            comment_text = comment.get('comment_text', '')\n",
    "            \n",
    "            # Extract stock symbols from comments\n",
    "            stock_symbols = extract_stock_mentions(comment_text)\n",
    "            \n",
    "            # Filter and normalize symbols\n",
    "            for symbol in stock_symbols:\n",
    "                if filter_valid_symbols(symbol):\n",
    "                    normalized_symbol = normalize_symbol(symbol)\n",
    "                    # Extra check to ensure no single-letter symbols get through\n",
    "                    if len(normalized_symbol) > 1:\n",
    "                        stock_adj_list[normalized_symbol].append(submission_id)\n",
    "            \n",
    "            # Process text normally\n",
    "            l_tokens, comment_words_dropped, comment_words = process_text(comment_text)\n",
    "            \n",
    "            # replace original text with tokenized version\n",
    "            comment['comment_text'] = ' '.join(l_tokens)\n",
    "            term_freq_counter.update(l_tokens)\n",
    "            total_words_dropped += comment_words_dropped\n",
    "            total_words_dataset += comment_words\n",
    "\n",
    "# Final check to remove any single-letter symbols that might have slipped through\n",
    "keys_to_remove = [k for k in stock_adj_list.keys() if len(k) == 1]\n",
    "for k in keys_to_remove:\n",
    "    del stock_adj_list[k]\n",
    "\n",
    "# Deduplicate submission IDs in the adjacency list\n",
    "for symbol in stock_adj_list:\n",
    "    stock_adj_list[symbol] = list(set(stock_adj_list[symbol]))\n",
    "\n",
    "# Save the tokenised data to a new JSON file\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(d_submissions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save the stock adjacency list to a separate JSON file\n",
    "stock_adj_output = f'../datasets/{subreddit_name}-stock-mentions.json'\n",
    "with open(stock_adj_output, 'w', encoding='utf-8') as f:\n",
    "    # Convert defaultdict to dict for serialization\n",
    "    stock_data = {\n",
    "        'stock_mentions': dict(stock_adj_list),\n",
    "        'mention_counts': {stock: len(submissions) for stock, submissions in stock_adj_list.items()}\n",
    "    }\n",
    "    json.dump(stock_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Amount of words in text fields: {total_words_dataset}\")\n",
    "print(f\"Amount of words dropped: {total_words_dropped}\")\n",
    "\n",
    "# Print out most common terms\n",
    "ranking = 1\n",
    "for term, count in term_freq_counter.most_common(freq_num):\n",
    "    print(f\"{ranking}: {term}: {count}\")\n",
    "    ranking += 1\n",
    "\n",
    "# Print statistics about stock mentions\n",
    "print(\"\\nStock Symbol Mentions:\")\n",
    "stock_mention_count = {symbol: len(submissions) for symbol, submissions in stock_adj_list.items()}\n",
    "for symbol, count in sorted(stock_mention_count.items(), key=lambda x: x[1], reverse=True)[:30]:\n",
    "    print(f\"{symbol}: {count} mentions\")\n",
    "\n",
    "print(f\"\\nTotal unique stock symbols/indices found: {len(stock_adj_list)}\")\n",
    "print(f\"Stock symbol data saved to: {stock_adj_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the most common terms,\n",
    "most_common = term_freq_counter.most_common(freq_num)\n",
    "\n",
    "# create histogram of term frequencies.\n",
    "terms, counts = zip(*most_common)\n",
    "\n",
    "# create a bar chart.\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.bar(range(len(terms)), counts, align='center')\n",
    "plt.xticks(range(len(terms)), terms, rotation=90)\n",
    "plt.xlabel('Terms')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Term Frequency Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# create a histogram of frequency values to show the distribution pattern.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=20)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of Terms')\n",
    "plt.title('Distribution of Term Frequencies')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  __Method and Modelling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentiment Analysis and Slang Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "custom_lexicon = {\n",
    "    # Bullish terms (positive)\n",
    "    'moon': 3.0,\n",
    "    'mooning': 3.0,\n",
    "    'bull': 2.0,\n",
    "    'bullish': 2.5,\n",
    "    'long': 1.0,\n",
    "    'calls': 1.5,\n",
    "    'hodl': 1.0,\n",
    "    'tendies': 2.0,\n",
    "    'rocket': 2.5,\n",
    "    'rocketship': 2.5,\n",
    "    'yolo': 1.5,\n",
    "    'buy the dip': 1.5,\n",
    "    'btd': 1.5,\n",
    "    'ath': 2.0,\n",
    "    'breakout': 1.5,\n",
    "    'outperform': 1.5,\n",
    "    'upgrade': 1.0,\n",
    "    'beat': 1.0,\n",
    "    \n",
    "    # Bearish terms (negative)\n",
    "    'bear': -2.0,\n",
    "    'bearish': -2.5,\n",
    "    'short': -1.0,\n",
    "    'puts': -1.5, \n",
    "    'bagholder': -2.0,\n",
    "    'sell off': -1.5,\n",
    "    'dump': -2.0,\n",
    "    'crash': -3.0,\n",
    "    'recession': -2.5,\n",
    "    'drilling': -2.0,\n",
    "    'tanking': -2.5,\n",
    "    'guh': -2.0,\n",
    "    'rugpull': -3.0,\n",
    "    'fud': -1.5,\n",
    "    'underperform': -1.5,\n",
    "    'downgrade': -1.0,\n",
    "    'miss': -1.0,\n",
    "}\n",
    "    \n",
    "\n",
    "stemmed_lexicon = {}\n",
    "for word, score in custom_lexicon.items():\n",
    "    stemmed_tokens, _, _ = process_text(word)\n",
    "    \n",
    "    # Join into a string if there are tokens\n",
    "    if stemmed_tokens:\n",
    "        stemmed_word = ' '.join(stemmed_tokens)\n",
    "        stemmed_lexicon[stemmed_word] = score\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Update VADER's lexicon with our stemmed custom lexicon\n",
    "sia.lexicon.update(stemmed_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Sentiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment(l_tokens):\n",
    "    \n",
    "    return sia.polarity_scores(l_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sentiment Analysis of Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "# import tokenised submission JSON file.\n",
    "input_filename = f'../datasets/{subreddit_name}-submissions-token.json'\n",
    "output_json = f'../datasets/{subreddit_name}-submissions-tf-idf.json'\n",
    "output_csv = f'../datasets/{subreddit_name}-submissions-tf-idf.csv'\n",
    "\n",
    "# Variables for text fields\n",
    "tokenised_submission = [\n",
    "    'submission_title',\n",
    "    'submission_text',\n",
    "]\n",
    "\n",
    "# Dictionary to store submission data\n",
    "submission_texts = {}\n",
    "submission_metadata = {}\n",
    "submission_topics = {}\n",
    "all_topics = set()\n",
    "\n",
    "# Open JSON file and process submissions\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    d_submissions = json.load(f)\n",
    "    \n",
    "    # Extract text from each submission and its comments\n",
    "    for submission in d_submissions['posts']:\n",
    "        submission_id = submission.get('submission_id', '')\n",
    "        combined_text = \"\"\n",
    "        sentiment_total = 0  # Renamed from combined_sentiment to avoid conflict\n",
    "        num_sentiment_items = 0\n",
    "        \n",
    "        # Get timestamp information\n",
    "        created_utc = submission.get('submission_date', '')\n",
    "        if created_utc:\n",
    "            try:\n",
    "                date_time = datetime.datetime.fromtimestamp(float(created_utc))\n",
    "            except:\n",
    "                date_time = None\n",
    "        else:\n",
    "            date_time = None\n",
    "        \n",
    "        # Get submission text fields\n",
    "        for var in tokenised_submission:\n",
    "            submission_text = submission.get(var, '')\n",
    "            if submission_text:\n",
    "                sentiment_score = compute_sentiment(submission_text)\n",
    "                print(sentiment_score['compound'])\n",
    "                sentiment_total += sentiment_score['compound']\n",
    "                num_sentiment_items += 1\n",
    "                combined_text += \" \" + submission_text\n",
    "        \n",
    "        # Get comment text fields\n",
    "        for comment in submission.get('submission_comments', []):\n",
    "            comment_text = comment.get('comment_text', '')\n",
    "            if comment_text:\n",
    "                sentiment_score = compute_sentiment(comment_text)  # Changed from combined_sentiment to compute_sentiment\n",
    "                sentiment_total += sentiment_score['compound']\n",
    "                num_sentiment_items += 1\n",
    "                combined_text += \" \" + comment_text\n",
    "        \n",
    "        # Calculate average sentiment\n",
    "        avg_sentiment = sentiment_total / num_sentiment_items if num_sentiment_items > 0 else 0\n",
    "        \n",
    "        # Store the combined text and metadata for this submission\n",
    "        submission_texts[submission_id] = combined_text.strip()\n",
    "        submission_metadata[submission_id] = {\n",
    "            'date': date_time.strftime('%Y-%m-%d') if date_time else '',\n",
    "            'time': date_time.strftime('%H:%M:%S') if date_time else '',\n",
    "            'timestamp': date_time.timestamp() if date_time else None,\n",
    "            'sentiment': avg_sentiment,\n",
    "            'num_comments': len(submission.get('submission_comments', [])),\n",
    "            'topics': []  # Will be filled after TF-IDF\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Topics of Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # create corpus from all submission texts.\n",
    "# corpus = list(submission_texts.values())\n",
    "# submission_ids = list(submission_texts.keys())\n",
    "\n",
    "# # create TF-IDF. \n",
    "# tfidf_vectorizer = TfidfVectorizer(\n",
    "#     # >80% of documents.\n",
    "#     max_df=0.8,  \n",
    "#     # min doc word count. \n",
    "#     min_df=3,\n",
    "#     # top 500 features.\n",
    "#     max_features=500,\n",
    "#     # ignore single words or phrases.\n",
    "#     ngram_range=(1, 2)\n",
    "# )\n",
    "\n",
    "# # fit the vectorizer to the corpus.\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# # get feature names.\n",
    "# feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "# print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "# # get top terms for each submission based on TF-IDF scores.\n",
    "# for i, submission_id in enumerate(submission_ids):\n",
    "#     # get the document vector.\n",
    "#     doc_vector = tfidf_matrix[i].toarray()[0]\n",
    "    \n",
    "#     # get indices of top 10 TF-IDF scores.\n",
    "#     top_indices = doc_vector.argsort()[-10:][::-1]\n",
    "    \n",
    "#     # get corresponding terms and their scores.\n",
    "#     top_terms_with_scores = [(feature_names[idx], doc_vector[idx]) for idx in top_indices if doc_vector[idx] > 0]\n",
    "#     top_terms = [term for term, score in top_terms_with_scores]\n",
    "\n",
    "#     # store as the topic for this submission\n",
    "#     submission_topics[submission_id] = top_terms\n",
    "#     # update metadata\n",
    "#     submission_metadata[submission_id]['topics'] = top_terms\n",
    "    \n",
    "#     # add to overall set of topics\n",
    "#     all_topics.update(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# Create corpus from all submission texts\n",
    "corpus = list(submission_texts.values())\n",
    "submission_ids = list(submission_texts.keys())\n",
    "\n",
    "# Create a document-term matrix using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.8,  # Ignore terms that appear in >80% of documents\n",
    "    min_df=3,    # Ignore terms that appear in <3 documents\n",
    "    max_features=500,  # Consider only top 500 features\n",
    "    ngram_range=(1, 2)  # Include single words and bigrams\n",
    ")\n",
    "\n",
    "# Fit the vectorizer to the corpus\n",
    "doc_term_matrix = count_vectorizer.fit_transform(corpus)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "# Set the number of topics for LDA\n",
    "num_topics = 15\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=num_topics,\n",
    "    max_iter=10,\n",
    "    learning_method='online',\n",
    "    random_state=42,\n",
    "    batch_size=128,\n",
    "    learning_decay=0.7\n",
    ")\n",
    "\n",
    "# Fit the model to the document-term matrix\n",
    "lda_model.fit(doc_term_matrix)\n",
    "\n",
    "# Function to print top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_terms = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(topic_terms)\n",
    "        print(f\"Topic #{topic_idx+1}: {', '.join(topic_terms)}\")\n",
    "    return topics\n",
    "\n",
    "# Print top words for each topic\n",
    "print(\"Topics found by LDA:\")\n",
    "all_topics_list = print_top_words(lda_model, feature_names)\n",
    "\n",
    "# Get topic distribution for each document\n",
    "topic_distributions = lda_model.transform(doc_term_matrix)\n",
    "\n",
    "# Assign the most probable topic to each submission\n",
    "submission_topics = {}\n",
    "for i, submission_id in enumerate(submission_ids):\n",
    "    # Get the most probable topic for this document\n",
    "    dominant_topic_idx = np.argmax(topic_distributions[i])\n",
    "    \n",
    "    # Get the top terms for this topic\n",
    "    top_terms = all_topics_list[dominant_topic_idx]\n",
    "    \n",
    "    # Store as the topic for this submission\n",
    "    submission_topics[submission_id] = top_terms\n",
    "    \n",
    "    # Update metadata\n",
    "    submission_metadata[submission_id]['topics'] = top_terms\n",
    "    \n",
    "    # Add top terms to overall set of topics\n",
    "    all_topics.update(top_terms)\n",
    "\n",
    "# Count the frequency of topics across all submissions\n",
    "topic_counts = {}\n",
    "for topics in submission_topics.values():\n",
    "    for topic in topics:\n",
    "        if topic in topic_counts:\n",
    "            topic_counts[topic] += 1\n",
    "        else:\n",
    "            topic_counts[topic] = 1\n",
    "\n",
    "# Print the most common topics\n",
    "print(\"\\nMost Common Topics:\")\n",
    "for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{topic}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Focused Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a topic-oriented view.\n",
    "topic_analysis = {}\n",
    "for topic in all_topics:\n",
    "    # find all submissions containing this topic.\n",
    "    relevant_submissions = [\n",
    "        sub_id for sub_id, topics in submission_topics.items() \n",
    "        if topic in topics\n",
    "    ]\n",
    "    \n",
    "    # calculate average sentiment for this topic.\n",
    "    sentiments = [submission_metadata[sub_id]['sentiment'] for sub_id in relevant_submissions if submission_metadata[sub_id]['sentiment'] != 0]\n",
    "    avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0\n",
    "    \n",
    "    # get timestamps for temporal analysis.\n",
    "    timestamps = [submission_metadata[sub_id]['timestamp'] for sub_id in relevant_submissions \n",
    "                 if submission_metadata[sub_id]['timestamp'] is not None]\n",
    "    \n",
    "    # store topic data.\n",
    "    topic_analysis[topic] = {\n",
    "        'count': len(relevant_submissions),\n",
    "        'avg_sentiment': avg_sentiment,\n",
    "        'submissions': relevant_submissions,\n",
    "        'earliest_mention': min(timestamps) if timestamps else None,\n",
    "        'latest_mention': max(timestamps) if timestamps else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame for easier analysis and graphing\n",
    "rows = []\n",
    "for submission_id, metadata in submission_metadata.items():\n",
    "    for topic in metadata['topics']:\n",
    "        rows.append({\n",
    "            'submission_id': submission_id,\n",
    "            'date': metadata['date'],\n",
    "            'time': metadata['time'],\n",
    "            'timestamp': metadata['timestamp'],\n",
    "            'sentiment': metadata['sentiment'],\n",
    "            'num_comments': metadata['num_comments'],\n",
    "            'topic': topic\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Save the DataFrame for later visualization\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Save the full analysis results to JSON\n",
    "results = {\n",
    "    'submission_metadata': submission_metadata,\n",
    "    'submission_topics': submission_topics,\n",
    "    'topic_analysis': {k: v for k, v in topic_analysis.items() if k},\n",
    "    'all_topics': list(all_topics)\n",
    "}\n",
    "\n",
    "print(f\"Found {len(all_topics)} unique topics across {len(submission_texts)} submissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Most Common Topics Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Most common topics\n",
    "topic_counts = Counter([topic for topics_list in submission_topics.values() for topic in topics_list])\n",
    "top_topics = topic_counts.most_common(15)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=[topic for topic, _ in top_topics], y=[count for _, count in top_topics])\n",
    "plt.title('Most Common Topics')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_topics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Most Common Topics Sentiment Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sentiment distribution by top topics\n",
    "top_topic_names = [topic for topic, _ in top_topics[:15]]\n",
    "topic_sentiments = df[df['topic'].isin(top_topic_names)]\n",
    "# Get topics in order of frequency\n",
    "ordered_topics = [topic for topic, _ in top_topics[:15]]\n",
    "\n",
    "# Use this order in the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='topic', y='sentiment', data=topic_sentiments, order=ordered_topics)\n",
    "plt.title('Sentiment Distribution by Topic')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_sentiments.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment for the Most Common Stocks Posted About"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# File paths\n",
    "stock_mentions_file = f'../datasets/{subreddit_name}-stock-mentions.json'\n",
    "submissions_metadata_file = '../datasets/stocks-submissions-tf-idf.json'\n",
    "\n",
    "# Load stock mentions data\n",
    "with open(stock_mentions_file, 'r', encoding='utf-8') as f:\n",
    "    stock_data = json.load(f)\n",
    "    stock_mentions = stock_data['stock_mentions']\n",
    "    mention_counts = stock_data['mention_counts']\n",
    "\n",
    "# Get top 25 most mentioned stocks/indices (increased from 15)\n",
    "top_stocks = sorted(mention_counts.items(), key=lambda x: x[1], reverse=True)[:25]\n",
    "top_stock_symbols = [symbol for symbol, count in top_stocks]\n",
    "\n",
    "print(f\"Analyzing sentiment for top 25 market symbols: {', '.join(top_stock_symbols)}\")\n",
    "\n",
    "# Load submissions metadata with sentiment data\n",
    "with open(submissions_metadata_file, 'r', encoding='utf-8') as f:\n",
    "    submissions_metadata = json.load(f)\n",
    "\n",
    "# Create a dictionary to store sentiment data for each stock\n",
    "stock_sentiment_data = defaultdict(list)\n",
    "\n",
    "# Process each submission ID to extract sentiment values\n",
    "for submission_id, metadata in submissions_metadata.items():\n",
    "    # Get sentiment value\n",
    "    sentiment = metadata.get('sentiment')\n",
    "    \n",
    "    # If sentiment value is available\n",
    "    if sentiment is not None:\n",
    "        # Check which of our top stocks are mentioned in this submission\n",
    "        for stock in top_stock_symbols:\n",
    "            if submission_id in stock_mentions.get(stock, []):\n",
    "                stock_sentiment_data[stock].append(float(sentiment))\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "sentiment_data = []\n",
    "for stock, sentiments in stock_sentiment_data.items():\n",
    "    for sentiment in sentiments:\n",
    "        sentiment_data.append({\n",
    "            'symbol': stock,\n",
    "            'sentiment': sentiment,\n",
    "            'mention_count': mention_counts[stock]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(sentiment_data)\n",
    "\n",
    "# Check if we have enough data\n",
    "if df.empty:\n",
    "    print(\"No sentiment data found for the top market symbols.\")\n",
    "else:\n",
    "    # Order stocks by mention count\n",
    "    ordered_stocks = [symbol for symbol, _ in top_stocks]\n",
    "    \n",
    "    # Count number of sentiment values we have for each stock\n",
    "    sentiment_counts = df['symbol'].value_counts().reindex(ordered_stocks, fill_value=0)\n",
    "    print(\"\\nNumber of sentiment values found for each market symbol:\")\n",
    "    for stock, count in sentiment_counts.items():\n",
    "        print(f\"{stock}: {count}\")\n",
    "    \n",
    "    # Only include stocks that have sentiment data\n",
    "    stocks_with_data = [stock for stock in ordered_stocks if stock in df['symbol'].unique()]\n",
    "    \n",
    "    if stocks_with_data:\n",
    "        # Calculate positive and negative sentiment counts for each symbol\n",
    "        sentiment_summary = []\n",
    "        for symbol in stocks_with_data:\n",
    "            symbol_data = df[df['symbol'] == symbol]\n",
    "            positive_count = len(symbol_data[symbol_data['sentiment'] > 0])\n",
    "            negative_count = len(symbol_data[symbol_data['sentiment'] < 0])\n",
    "            neutral_count = len(symbol_data[symbol_data['sentiment'] == 0])\n",
    "            \n",
    "            sentiment_summary.append({\n",
    "                'symbol': symbol,\n",
    "                'positive': positive_count,\n",
    "                'negative': negative_count,\n",
    "                'neutral': neutral_count,\n",
    "                'total': len(symbol_data),\n",
    "                'pos_pct': positive_count / len(symbol_data) * 100 if len(symbol_data) > 0 else 0,\n",
    "                'neg_pct': negative_count / len(symbol_data) * 100 if len(symbol_data) > 0 else 0,\n",
    "                'neu_pct': neutral_count / len(symbol_data) * 100 if len(symbol_data) > 0 else 0\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(sentiment_summary)\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # Sort by total mentions for better visualization\n",
    "        summary_df['mentions'] = summary_df['symbol'].map(mention_counts)\n",
    "        summary_df = summary_df.sort_values('mentions', ascending=False)\n",
    "        \n",
    "        # Set up the bar positions\n",
    "        bar_width = 0.35\n",
    "        x = np.arange(len(summary_df))\n",
    "        \n",
    "        # Create the grouped bars\n",
    "        ax = plt.subplot(111)\n",
    "        pos_bars = ax.bar(x - bar_width/2, summary_df['positive'], bar_width, \n",
    "                         color='green', alpha=0.7, label='Positive')\n",
    "        neg_bars = ax.bar(x + bar_width/2, summary_df['negative'], bar_width, \n",
    "                         color='red', alpha=0.7, label='Negative')\n",
    "        \n",
    "        # Add count labels above bars\n",
    "        for bar in pos_bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                        f'{int(height)}', ha='center', va='bottom', \n",
    "                        fontsize=9, fontweight='bold')\n",
    "        \n",
    "        for bar in neg_bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                        f'{int(height)}', ha='center', va='bottom',\n",
    "                        fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Add labels, title and legend\n",
    "        plt.title('Positive vs Negative Sentiment Counts for Top 25 Market Symbols', fontsize=16)\n",
    "        plt.xlabel('Market Symbol', fontsize=14)\n",
    "        plt.ylabel('Count', fontsize=14)\n",
    "        plt.xticks(x, summary_df['symbol'], rotation=45, ha='right', fontsize=11)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        # Add total mention counts below each symbol\n",
    "        for i, (_, row) in enumerate(summary_df.iterrows()):\n",
    "            plt.text(i, -5, f'n={row[\"mentions\"]}', ha='center', fontsize=9)\n",
    "        \n",
    "        # Add some space at the bottom for the mention counts\n",
    "        plt.ylim(bottom=-10)\n",
    "        \n",
    "        # Add grid lines for easier reading\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Tight layout to avoid clipping labels\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig('market_symbol_pos_neg_sentiment_counts.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a table with the data for reference\n",
    "        print(\"\\nPositive vs Negative Sentiment Counts by Market Symbol:\")\n",
    "        display_df = summary_df[['symbol', 'positive', 'negative', 'total', 'pos_pct', 'neg_pct', 'mentions']]\n",
    "        display_df = display_df.rename(columns={\n",
    "            'symbol': 'Symbol',\n",
    "            'positive': 'Positive', \n",
    "            'negative': 'Negative',\n",
    "            'total': 'Total Sentiments',\n",
    "            'pos_pct': 'Positive %',\n",
    "            'neg_pct': 'Negative %',\n",
    "            'mentions': 'Total Mentions'\n",
    "        })\n",
    "        print(display_df.to_string(index=False, float_format=lambda x: f'{x:.1f}'))\n",
    "        \n",
    "        # Save the data to CSV\n",
    "        display_df.to_csv('market_symbol_sentiment_counts.csv', index=False)\n",
    "        print(\"\\nData saved to market_symbol_sentiment_counts.csv\")\n",
    "    else:\n",
    "        print(\"No market symbols with sentiment data found in the top 25.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Topics Highest and Lowest Rated Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3. Print min and max sentiment submission IDs for top 5 topics\n",
    "print(\"\\n--- Min/Max Sentiment Submission IDs for Top 5 Topics ---\")\n",
    "for topic, count in top_topics[:5]:\n",
    "    print(f\"\\nTopic: {topic} (Count: {count})\")\n",
    "    \n",
    "    # Get relevant submissions and their sentiment scores\n",
    "    topic_submissions = []\n",
    "    for sub_id in topic_analysis[topic]['submissions']:\n",
    "        sentiment = submission_metadata[sub_id]['sentiment']\n",
    "        date = submission_metadata[sub_id]['date']\n",
    "        topic_submissions.append((sub_id, sentiment, date))\n",
    "    \n",
    "    # Skip if no submissions with valid sentiment scores\n",
    "    if not topic_submissions:\n",
    "        print(\"  No submissions with sentiment scores found\")\n",
    "        continue\n",
    "    \n",
    "    # Find min and max sentiment submissions\n",
    "    min_submission = min(topic_submissions, key=lambda x: x[1])\n",
    "    max_submission = max(topic_submissions, key=lambda x: x[1])\n",
    "    \n",
    "    # Print min sentiment submission\n",
    "    print(f\"  Minimum Sentiment:\")\n",
    "    print(f\"    Submission ID: {min_submission[0]}\")\n",
    "    print(f\"    Sentiment Score: {min_submission[1]:.4f}\")\n",
    "    print(f\"    Date: {min_submission[2]}\")\n",
    "    \n",
    "    # Print max sentiment submission\n",
    "    print(f\"  Maximum Sentiment:\")\n",
    "    print(f\"    Submission ID: {max_submission[0]}\")\n",
    "    print(f\"    Sentiment Score: {max_submission[1]:.4f}\")\n",
    "    print(f\"    Date: {max_submission[2]}\")\n",
    "    \n",
    "    # Create sentiment trend graph\n",
    "    topic_df = df[df['topic'] == topic].sort_values('timestamp')\n",
    "    topic_df['datetime'] = pd.to_datetime(topic_df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Word Clouds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty dictionary for word cloud\n",
    "top_topics = topic_counts.most_common(150)\n",
    "wordcloud_topics = {}\n",
    "\n",
    "# Your topic data using frequency for size and sentiment for color\n",
    "for topic, count in top_topics:\n",
    "    # Use the count (frequency) as the value for sizing\n",
    "    wordcloud_topics[topic] = count  # This will make size based on frequency\n",
    "\n",
    "# Get sentiment values for coloring\n",
    "sentiment_dict = {topic: topic_analysis[topic]['avg_sentiment'] for topic, _ in top_topics}\n",
    "min_sentiment = min(sentiment_dict.values())\n",
    "max_sentiment = max(sentiment_dict.values())\n",
    "\n",
    "# Color function based on sentiment - red to yellow to green\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    sentiment = sentiment_dict.get(word, 0)\n",
    "    # Normalize sentiment to [0,1]\n",
    "    normalized = (sentiment - min_sentiment) / (max_sentiment - min_sentiment) if max_sentiment > min_sentiment else 0.5\n",
    "    \n",
    "    # Convert to a color (red to yellow to green)\n",
    "    if normalized < 0.5:\n",
    "        # Red to yellow (bad to neutral)\n",
    "        # Increase green component as we move toward middle\n",
    "        return f\"rgb(255, {int(255 * normalized * 2)}, 0)\"\n",
    "    else:\n",
    "        # Yellow to green (neutral to good)\n",
    "        # Decrease red component as we move toward good\n",
    "        return f\"rgb({int(255 * (1 - (normalized - 0.5) * 2))}, 255, 0)\"\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    background_color='white',\n",
    "    color_func=color_func,  # Custom color function based on sentiment\n",
    "    prefer_horizontal=0.9,  # Allow some vertical words for better packing\n",
    "    max_words=100,\n",
    "    relative_scaling=0.6,  # Balance frequency vs. ranking\n",
    "    min_font_size=8,\n",
    "    max_font_size=80,\n",
    "    random_state=42  # Fixed seed for reproducibility\n",
    ").generate_from_frequencies(wordcloud_topics)\n",
    "\n",
    "# Create a figure for the word cloud\n",
    "plt.figure(figsize=(12, 8), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# Add a title and explain the color scheme\n",
    "plt.title(\"Topic Frequency with Sentiment Color\", fontsize=16, pad=20)\n",
    "\n",
    "# Add a small color legend\n",
    "ax = plt.gca()\n",
    "ax.text(0.01, 0.01, \"Red = Negative Sentiment\", transform=ax.transAxes, \n",
    "        fontsize=10, color='darkred', ha='left')\n",
    "ax.text(0.5, 0.01, \"Yellow = Neutral Sentiment\", transform=ax.transAxes, \n",
    "        fontsize=10, color='goldenrod', ha='center')\n",
    "ax.text(0.99, 0.01, \"Green = Positive Sentiment\", transform=ax.transAxes, \n",
    "        fontsize=10, color='darkgreen', ha='right')\n",
    "\n",
    "# Save the generated image as a PNG file\n",
    "wordcloud.to_file(\"topic_frequency_wordcloud.png\")\n",
    "print(\"Word cloud saved as 'topic_frequency_wordcloud.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Based on your output, we know the top topics and their terms\n",
    "# Let's hardcode these since we already have them\n",
    "top_topic_indices = [7, 1, 2, 9, 3]\n",
    "top_terms_by_topic = {\n",
    "    7: [\"buy\", \"invest\", \"time\", \"money\", \"sell\", \"long\", \"hold\", \"etf\"],\n",
    "    1: [\"compani\", \"price\", \"buy\", \"share\", \"earn\", \"growth\", \"revenu\", \"go\"],\n",
    "    2: [\"tariff\", \"trump\", \"go\", \"economi\", \"peopl\", \"time\", \"recess\", \"day\"],\n",
    "    9: [\"compani\", \"ai\", \"use\", \"googl\", \"peopl\", \"time\", \"work\", \"need\"],\n",
    "    3: [\"tesla\", \"car\", \"elon\", \"musk\", \"compani\", \"tsla\", \"buy\", \"peopl\"]\n",
    "}\n",
    "\n",
    "# We'll create a simpler visualization without t-SNE since we don't have the full model data\n",
    "# This will use a force-directed layout algorithm to position terms\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Create a colormap for topics\n",
    "colors = sns.color_palette(\"hsv\", len(top_topic_indices))\n",
    "topic_colors = {topic_idx: colors[i] for i, topic_idx in enumerate(top_topic_indices)}\n",
    "\n",
    "# We'll create a simple force-directed layout manually\n",
    "# First, assign random initial positions\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Collect all unique terms\n",
    "all_terms = []\n",
    "term_to_topic = {}\n",
    "\n",
    "for topic_idx, terms in top_terms_by_topic.items():\n",
    "    for term in terms:\n",
    "        if term not in all_terms:\n",
    "            all_terms.append(term)\n",
    "            term_to_topic[term] = topic_idx\n",
    "        # If term appears in multiple topics, we'll assign it to the last one\n",
    "        # We could also weight this decision, but for simplicity we'll just take the last\n",
    "\n",
    "# Count term occurrences across topics (to use for sizing)\n",
    "term_counts = {}\n",
    "for terms in top_terms_by_topic.values():\n",
    "    for term in terms:\n",
    "        if term in term_counts:\n",
    "            term_counts[term] += 1\n",
    "        else:\n",
    "            term_counts[term] = 1\n",
    "\n",
    "# Create positions for all terms\n",
    "term_positions = {term: np.random.rand(2) * 20 - 10 for term in all_terms}\n",
    "\n",
    "# Simple force-directed layout simulation\n",
    "# Terms from the same topic attract, terms from different topics repel\n",
    "def apply_forces(positions, iterations=50):\n",
    "    for _ in range(iterations):\n",
    "        forces = {term: np.zeros(2) for term in positions}\n",
    "        \n",
    "        # Apply attractive forces between terms in same topic\n",
    "        for topic_idx, terms in top_terms_by_topic.items():\n",
    "            for i, term1 in enumerate(terms):\n",
    "                for term2 in terms[i+1:]:\n",
    "                    if term1 in positions and term2 in positions:\n",
    "                        # Vector from term1 to term2\n",
    "                        vec = positions[term2] - positions[term1]\n",
    "                        distance = np.linalg.norm(vec)\n",
    "                        if distance > 0:\n",
    "                            # Attractive force (like a spring)\n",
    "                            force = vec * 0.05  # Spring constant\n",
    "                            forces[term1] += force\n",
    "                            forces[term2] -= force\n",
    "        \n",
    "        # Apply repulsive forces between terms in different topics\n",
    "        for i, term1 in enumerate(all_terms):\n",
    "            for term2 in all_terms[i+1:]:\n",
    "                if term_to_topic[term1] != term_to_topic[term2]:\n",
    "                    # Vector from term1 to term2\n",
    "                    vec = positions[term2] - positions[term1]\n",
    "                    distance = np.linalg.norm(vec)\n",
    "                    if distance > 0:\n",
    "                        # Repulsive force (inverse square law)\n",
    "                        force = vec / (distance**2) * 10  # Repulsion constant\n",
    "                        forces[term1] -= force\n",
    "                        forces[term2] += force\n",
    "        \n",
    "        # Update positions based on forces\n",
    "        for term in positions:\n",
    "            # Limit force magnitude to prevent instability\n",
    "            force_mag = np.linalg.norm(forces[term])\n",
    "            if force_mag > 2:\n",
    "                forces[term] = forces[term] / force_mag * 2\n",
    "            \n",
    "            positions[term] += forces[term]\n",
    "    \n",
    "    return positions\n",
    "\n",
    "# Run simulation\n",
    "term_positions = apply_forces(term_positions, iterations=100)\n",
    "\n",
    "# Normalize positions to fit nicely in figure\n",
    "all_pos = np.array(list(term_positions.values()))\n",
    "min_pos = all_pos.min(axis=0)\n",
    "max_pos = all_pos.max(axis=0)\n",
    "for term in term_positions:\n",
    "    term_positions[term] = (term_positions[term] - min_pos) / (max_pos - min_pos) * 16 - 8\n",
    "\n",
    "# Draw topic areas first (underneath)\n",
    "for topic_idx in top_topic_indices:\n",
    "    # Get positions of all terms in this topic\n",
    "    topic_terms = top_terms_by_topic[topic_idx]\n",
    "    positions = np.array([term_positions[term] for term in topic_terms if term in term_positions])\n",
    "    \n",
    "    if len(positions) > 0:\n",
    "        # Calculate the center of the terms\n",
    "        center = positions.mean(axis=0)\n",
    "        \n",
    "        # Calculate radius to encompass all terms\n",
    "        radius = max(np.linalg.norm(positions - center, axis=1)) * 1.3\n",
    "        \n",
    "        # Draw a circle for this topic area\n",
    "        circle = plt.Circle(center, radius, color=topic_colors[topic_idx], alpha=0.2)\n",
    "        plt.gca().add_patch(circle)\n",
    "        \n",
    "        # Add topic label\n",
    "        plt.text(center[0], center[1], f\"Topic {topic_idx}\", \n",
    "                 fontsize=14, fontweight='bold', ha='center', va='center',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Draw connections between terms in the same topic\n",
    "for topic_idx, terms in top_terms_by_topic.items():\n",
    "    # Draw connections between terms in the same topic\n",
    "    for i, term1 in enumerate(terms):\n",
    "        for term2 in terms[i+1:]:\n",
    "            if term1 in term_positions and term2 in term_positions:\n",
    "                x1, y1 = term_positions[term1]\n",
    "                x2, y2 = term_positions[term2]\n",
    "                \n",
    "                # Draw line with weight based on term commonality\n",
    "                line_weight = (term_counts[term1] + term_counts[term2]) / 8\n",
    "                plt.plot([x1, x2], [y1, y2], '-', color=topic_colors[topic_idx], \n",
    "                         alpha=0.4, linewidth=0.5 + line_weight)\n",
    "\n",
    "# Draw each term\n",
    "for term in all_terms:\n",
    "    if term in term_positions:\n",
    "        topic_idx = term_to_topic[term]\n",
    "        x, y = term_positions[term]\n",
    "        \n",
    "        # Size based on term frequency across topics\n",
    "        size = 100 + 300 * term_counts[term]\n",
    "        \n",
    "        # Plot the term node\n",
    "        plt.scatter(x, y, s=size, color=topic_colors[topic_idx], alpha=0.7, \n",
    "                   edgecolors='black', linewidth=1, zorder=10)\n",
    "        \n",
    "        # Add term label\n",
    "        plt.text(x, y, term, fontsize=10, ha='center', va='center', \n",
    "                 fontweight='bold' if term_counts[term] > 1 else 'normal',\n",
    "                 zorder=11)\n",
    "\n",
    "# Add legend for topics\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', \n",
    "                          markerfacecolor=topic_colors[topic_idx], markersize=10,\n",
    "                          label=f'Topic {topic_idx}: {\", \".join(top_terms_by_topic[topic_idx][:3])}...')\n",
    "                  for topic_idx in top_topic_indices]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=2)\n",
    "\n",
    "plt.title('Term Correlation Map for Top Topics', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
